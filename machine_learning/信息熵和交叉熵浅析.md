# 信息熵和交叉熵的浅析
信息熵：
$$
H(P) = -\sum p_i \log p_i
$$
熵越大，说明系统越稳定，说明出现各种情况的概率越均衡，如果变量x有n中可能取值，熵越大，各种取值概率越接近，也接近$\frac{1}{n}$，最大熵的时候，就是各种取值为$\frac{1}{n}$，最大熵模型也就是找最均衡的概率模型，比如对抛筛子，各个面出现的概率，进行建模，那么最大熵模型就是个面个概率取相同为1/6。这也符合实际，所以最大熵模型是个不错的模型。  
信息熵也用来衡量surprise的程度，越小，说明可能是某个取值概率极大，其他极小，像买彩票中奖的概率。  
交叉熵和KL散度：
$$
H(p,q) = H(p) + D_{KL}(p||q)
$$
KL散度是描述两个概率模型散开的程度，反过来说是两个概率模型重合的程度，重合度越好，散度越小，但是无疑的是，散度是>=0的，因为只要p不等于q，那么两者就有不重合的部分，那么散度就大于0。而交叉熵是两者的和，对于概率分布p来说，只有当拟合的概率模型q越接近p，散度越小，交叉熵才能越小，所以交叉熵越小，也反应建模的概率q越接近真是概率分布p。  
但是因为不知道真实的概率分布p，我们用经验分布来代替，那么经验分布如何取？我们现在只考虑离散变量，我们要模拟的概率模型不只有一个，K个分类就有K个模型，每个模型的经验分布是什么呢？~~将每个样本集单独看待，对离散模型，其经验分布是各类别出现的个数除以总个数，以第一个样本为例，假设类别是k，那意思就是总出现类别的个数是1，除了第k类，其经验概率就是除了第k个位置为1外，其他皆为0的向量。~~ 该理解不是很到位，因为那样来理解的话，应该只有一个经验分布的，现在每一个样本，只要分到的类别不同，得到的经验分布就不同，那模拟的到底和哪一个经验分布呢？还是像混合高斯模型那样理解，有K类，那么每一类的经验分布分别是各自的one-hot，也就是取到本类别，概率为1，否则为0。这样，当某一个样本标签是k时，那么就是对选择第k个的概率模型进行交叉熵。  
当然交叉熵也可以用最大似然概率来推导得到，推导很简单，就是对每一个样本而言，标签是几，那么就选择预测它的概率，以此类推，将选择的这些概率连乘(最大似然)，然后取负对数就得到整个训练集合交叉熵了。

