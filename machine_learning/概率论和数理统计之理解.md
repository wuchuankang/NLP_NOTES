## 概率论和数理统计之理解
这是对概率论和数理统计知识中模糊不清的部分进行梳理，明确并加深理解
### 概率论

* 概率论中讨论的对象是变量，变量有一维变量和多维变量，其中多维变量的每一个元素都是一个一维变量。
* 概率论中关注的是变量的数字特征  
    1. 期望：多维变量的期望由其中每一个元素的期望构成
    2. 方差：只是对一维变量而言的，$var(x)=E[(x-E(x))^2]$
    3. 协方差：考察的是两个一维变量之间的相关性，$cov(x,y)=E[(x-E(x))(y-E(y))]$
    4. 协方差矩阵：对多维变量而言
    $$
    C = 
    \left[
    \begin{matrix}
   cov(x_1,x_1)&...&cov(x_1,x_n)\\
       ...&... &...\\
       cov(x_n,x_1)&...&cov(x_n,x_n)
      \end{matrix} 
    \right]
    $$
### 数理统计

* 讨论的对象是样本，这里的样本可以看成是训练集，有时候，也称单个训练集是样本。样本是从总体中抽样得到的，可以认为是总体的一部分。总体变量的维度决定了样本的维度。
* 关注的是样本的统计量：  
    1. 均值
    2. 方差：其无偏估计的样本方差为 $\frac{1}{m-1}\sum_{i=1}^m (x_i-\bar x)^2$
    3. 协方差：两个一维样本之间的相关性，$\frac{1}{m-1}\sum_{i=1}^m (x_i-\bar x)(y_i-\bar y)$
    4. 协方差矩阵：对于多维变量的样本而言的，具体说明一下，假设总体是多维的$(X_1,X_2,...,X_n)$，样本的观察值为$\{(x_1^1,x_2^1,...,x_n^1),...,(x_1^m,x_2^m,...,x_n^m)\}$，因为协方差矩阵中要考察任意两个变量（特征）之间的相关性，所以其元素就是有任意两个变量的协方差。
    $$
   b_{ij} =  \frac{1}{m-1}\sum_{k=1}^m (x_i^k-\bar x_i)(x_j^k-\bar x_j)
    $$
    5. 在PCA中，我们经常会碰到给定样本的协方差矩阵是：
    $$
   var(x) = \frac{1}{m-1} X^TX
    $$
    其中$X$的每一行是一个样本，$X \in R^{m\times n}$，为什么是这样呢？
    其实很容易，那就是我们预处理中将所有样本减去了均值。

### 两者之间的关系
实际应用中，是通过数理统计对概率理论进行近似处理的，也就是如果想获得给定训练集所服从的概率分布的数字特征，用的是样本的统计量来代替的，当然统计量只是近似，首先观察样本(训练集)有限，一般是无法知道样本真实的概率分布的。

### 再次思考PCA
要考虑`PCA`，首先要考虑特征值和特征向量的含义。  
`n`阶实对称阵可以看成是由若干个行向量或者是列向量组成，假设是列向量，这些列向量所在张成的空间中，特征向量就是空间的基(可以理解成坐标系)，特征值就是该矩阵在该空间所在基上的数值长度。长度越长，说明该方向所占信息比重越大。在 `PCA` 中，我们的目的找到数据尽可能分散的方向，也就是前面所说的在基上数值最大的基。但是我们应该找哪一个实对称阵呢？数据的分散程度是用方差或者协方差来说明的，前者变量维度是1维，当变量是多维的时候，那么就是协方差。所以就是协方差矩阵。  取协方差矩阵前 `k` 个大的方向向量，按列排列构成 `(n,k)` 矩阵S，原先的 `X` (维度`(m,n)`)布局是每一行是一个样本输入，每个样本维度是 `n` ，那么通过 `XS` 就可以将所有的样本的维度降低到 `k`维度。  
这里简要的说明一下，为什么是$X^TX$，而不是 $XX^T$,前者的得到的维度是 `(n,n)`， `n`是维度大小，是我们所要进行降低的维度，具有实际意义，后者是 `(m,m)`，显然后续的意义不存在。这只是简单快速记忆的方法。
