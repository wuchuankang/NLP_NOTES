## 降维和升维
    降维和升维是机器学习中很重要的一块，都是对特征进行了转化。降维包括特征抽取和特征选择，也可以说是表示学习（特征学习，去除冗余的特征），以降低学习难度。升维是将特征空间映射到高高维空间、甚至是无限维中（径向基函数），只要依靠的手段是核函数。
### 降维：PCA和SVD

    降维的思想实际就是源于矩阵的特征值分解，大的特征值对应的特征性向量含有更多的信息，小的特征向量可以认为是冗余向量，保留前k个大的特征值对应的特征向量，就实现了降维。
#### PCA
* 那么现在给定一个具有m个样本的训练集$\{(x_1^1,x_2^1,...,x_n^1),...,(x_1^n,x_2^n,...,x_n^m)\}$，我们如何特征提取，以实现降维呢？这里不从具体的技术细节上实现推导过程，我们从更高的角度来理解，这样才能记忆的更深刻。我们将每一个样本作为横向量堆起来，构成一个$m\times n$的向量，所有的信息都存在这个矩阵中，我们需要通过一定的方式把重要的信息提取出来，有没有这样的手段呢？协方差矩阵！协方差矩阵正是体现了各个特征之间的相关性，各个特征耦合在一起的信息都在这个矩阵中了。如果我们把这个矩阵的前k个大特征值对应的特征向量找出来，那么就可以实现将特征维度降到k维的目的了。
* 为了后续处理的方便，进行这样的预处理，将每一个样本减去均值，这样，方差矩阵就可以写成一下的形式：  
$$
var(x) = \frac{1}{m-1} X^TX=\frac{1}{m-1}\sum_{i=1}^m \bold x_i \bold x_i^T  \tag{1}
$$
* 我们要提取$(1)$中协方差矩阵的特征向量，对前面的系数是可以不用考虑的。由于$X^TX$是实对称阵，所以可以进行特征值分解： $X^TX = U\Lambda U^T$，其中U是正交阵，即：$UU^T=I$。在求解特征值和特征向量后，将特征值从大到小排列，对应的特征向量在$U$中也要对应的调整位置，现在我门取前k个特征值对应的特征向量，然后进行下面的线性变换，就可将原来的n维降到k维：
$$
\left[
    \begin{matrix}
    u_1^T\\
        u_2^T\\
        \vdots\\
        u_k^T
    \end{matrix}
\right]_{k\times n}
\bold x_{n\times 1}
=\left[
    \begin{matrix}
    z_1\\
        z_2\\
        \vdots\\
        z_k
    \end{matrix}
\right]_{k\times 1}  \tag{2}
$$
* 如果把维度降到3维或者2维，就可以实现可视化，就可以直观感受出效果是什么样的。
* PCA的一个重要特性是将数据变为彼此之间互不相关，就是线性独立的，这不是说$Z_i^TZ_j=0$，因为线性独立并不是正交，正交是一个更强的概念，那么如何判断独立呢？也就是线性无关呢？首先从宏观角度，所有的样本经过一个元素为正交向量的矩阵线性变换，结果应该是线性无关的。现在着眼于线性无关，那什么体现相关性呢？显然是协方差矩阵，如果我们将协方差矩阵是个对角阵，那么显然无关的了。
$$
Z^TZ = U_kX^TXU_k^T=(U_kU)\Lambda(U_kU)^T=\Sigma   \tag{3}
$$
　　为什么认为最终结果$\Sigma$是个对角阵呢？首先，如果k不是只取前个，而是所有，那么$Z^TZ=\Lambda$，现在取了前k个，无非是$\Sigma$的秩少一些而已，还是个对角阵，问题得证。  
* 上面直接对原始特征进行PCA分析的，但也会遇到对映射特征（通过基函数将原始特征映射）来进行PCA分析，这被称为主成分分析。  
