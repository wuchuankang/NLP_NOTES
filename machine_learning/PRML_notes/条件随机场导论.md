## 条件随机场导论

[TOC]

###  1. 简介

​	许多应用的根本是要由预测相互依赖的多变量的能力。这样的应用是多种多样的，比如图片区域的划分，估计围棋游戏的分数，对一段DNA进行分割，从自然语言中提取语法。在这些应用中，给定观测特征向量$\bold x$，我们希望能够预测出一个随机向量变量$\bold y=[y_0,y_1,...,y_T]$。自然语言处理一个相对简单的例子是词性标注，其中，每一个变量$y_s$是在位置$s$处单词的词性标签，而输入变量 $\bold x$（$\bold x$是被标注的一句话）被分成一个特征向量$[\bold x_0,\bold x_1,...,\bold x_T]$。每一个$\bold x_s$都包含了在位置$s$处单词的各种信息，比如它的标识、拼写的正确性（比如前后缀）等等。

​	这种多变量预测问题的一种方法是学习一个独立与位置无关的分类器，使得对每一个$s$，都有一个$\bold x \rightarrow y$的映射，尤其是当我们的目标是最大化标签$y_s$ 正确分类的个数的时候，用这样的方法。但是困难在于输出变量有复杂的依赖关系。例如文献中相邻的词或一张图片中相邻的区域倾向于有相似的标签。又或者输出变量可能具有相当复杂的结构，比如解析树，其中，这个树顶层附近语法规则的选择对树的其余部分有较大的影响。

​	对于相互依赖的输出变量，一种自然的表示这种行为的方法是图模型。图模型多种多样，包括贝叶斯网络、神经网络、因子图、马尔科夫随机场、Ising models等等，它能够用变量子集上局部因子乘积的形式，来表示一个多变量的复杂分布。由此，它可以用来描述一个给定的概率密度的因式分解是如何对应与该分布所满足的条件独立的特定集合的。这种对应性使得建模更容易，因为通常我们对于某一领域的知识能够给出合理的条件独立性假设，这种假设决定了我们对因子的选择。

​	学习中利用图模型的许多工作，尤其是在统计自然语言处理中，都将重点放在生成模型中，生成模型是尝试对联合概率分布$p(\bold y, \bold x)$进行建模。尽管这种方法有很多优势，但它也有很强的限制。不仅是$\bold x$的维度非常大，往往特征之间具有复杂的依赖性，所以构建这样的一个概率分布是很难的。对输入变量的依赖性进行建模会导致复杂的模型，但是忽略依赖性又会导致性能变差。

​	一种解决途径是直接对条件概率$p(\bold y|\bold x)$进行建模，对于分类问题直接这样建模就足够了。直接对条件概率建模就是一个条件随机场。条件随机场本质上是一种结合了分类和图模型优势、结合了对多变量数据紧致建模和平衡较大输入变量的能力。条件概率模型的优点是只涉及$\bold x$变量的依赖性在条件模型中不起作用，所以一个精确的条件模型会比联合概率模型具有相当简单的结构形式。生成模型和CRFs模型之间的差异可以类比于朴素贝叶斯模型和Logistic  回归模型之间的差异。事实上，多变量logstic 回归模型可以被看做是最简单的CRF，其中只有一个输出变量。

​	有大量的场景会用到CRFs。成功的应用有文本处理，生物信息统计，计算机视觉。CRFs较早的时候采用线性链结构，现在多采用更加一般的图结构。一般的图结构在预测复杂的结构的时候有用，例如应用在图和树结构。另外在相关学习中，通过放宽独立同分布假设，也会取得较好的结果。

​	这份教程叙述了利用条件随机场进行建模、推断、和参数学习的过程。我们假定没有图模型的先验知识，所以这份教程对广泛领域的实践者很有用。我们在第二章叙述建模问题，包括线性链CRFs、一般图结构的CRFs和具有隐变量的隐CRFs。我们会说明CRFs既可以被看做是Logistic 线性回归的泛化，也可以作为一个判别模型和HMM模型（生成模型）进行类比。 

​	在接下来的两章，我们将说明CRFs的推断（第3章）和学习（第4章）。推断和学习这两个流程是紧密相关的，因为学习通常会调用推断作为一个子程序。对于图模型来说，尽管我们讨论的推断算法是标准的算法，但由于推断算法是内嵌在一个外层是参数估计的流程中，这就会引起额外的问题。最后，我们讨论CRFs和其他图模型族之间的关系，包括结构方法、神经网络、最大熵马尔科夫模型（第5章）。

#### 实现细节

​	通过这个专题，我们尝试指出研究文献中有时候会省略的实现细节。例如我们讨论与特征工程相关的问题（第2.6节）、在推断过程中避免数值外溢（第3.3节）和CRF在基准问题上训练的规模问题。

​	因为这是我们章节中首次提到细节的实现，所以在这里介绍一些CRFs可用的实现是合适的。几个比较流行的实现是：

 + CRF++

 + MALLET

 + GRMM

 + CRFSuite

 + FACTORIE

   同时，用于马尔科夫逻辑网络的软件可以用来构建CRF 模型。

### 2. 建模

#### 2.1 图模型

​	对与多变量概率分布的表示和推断，图模型是一个有用的框架。在随机建模的多个领域，都已被证实是很有用的。包括编码理论、计算机视觉、知识表示、贝叶斯统计和自然语言处理。

​	多变量分布如果朴素的表示的话，会比较费事。例如，对于$n$个二值变量的联合概率需要用$O(2^n)$个浮点数来存储。图模型观点的本质是：一个多变量分布可以用相互依赖的变量子集所构成的函数的乘积来表示。这种因式分解说明了与变量之间的条件依赖具有很强的联系。事实上，因式分解、条件依赖和图结构之间的关系组成了图模型框架能力：条件依赖的观点在设计模型的时候最有用，因式分解的观点在设计推断算法的时候最有用。

​	在其余的小节中，我们从因式分解和条件依赖的角度引入了图模型，并关注基于无向图的模型。关于图模型和近似推断更加详细、流行观点参照Koller和Friedman的书。

##### 2.1.1 无向图模型

​	我们来考虑关于随机变量集合$V=X\bigcup Y$ 的概率分布，其中$X$ 是我们观测到的输入变量，$Y$是我们要预测的输入变量。每一个变量$s \in V$ 都是来自于集合$V$，变量$s$既可以是连续型也可是离散型，在该教程中我们只考虑离散型。$X$是一个向量，对其赋值，就意味着赋一个向量$\bold x$。给定一个变量$s\in X$，符号$x_s$ 意思是赋给向量$\bold x$在$s$位置处元素的值。类似地，对于一个子集$a\subset X$，$\bold x_a$意味着该子集所赋的值。符号$\bold 1_{\left \{ x=x' \right \}}$ 是指示函数，当$x=x'$时取为1，其他情况为0。我们还需要对边缘进行标识。对于一个赋值为$y_s$的固定变量来说，我们用$\sum_{\bold y  \diagdown  y_s}$ 来表示对第$s$个元素变量等于$y_s$的所有可能的$\bold y$的求和。

​	假定感兴趣的概率分布$p$可用$\Psi_a(\bold x_a,\bold  y_a)$形式的因式乘积来表示，其中$a \subseteq V$ 。因为子集$a$可能比变量全集$V$要小很多，所以这种因式分解来表示分布$p$更高效。不失一般性

，我们假定每一个独立的子集$a$最多有一个因子式$\Psi_a$ 。

​	一个无向图模型是这样的一种概率分布族，这种概率分布的因式分解是根据给定范围的集合。正式的说，给定一个子集集合$F=a \subset V$，任意一个分布都可以用一个写成如下形式的无向图模型来定义：
$$
p(\bold x, \bold y)=\frac{1}{Z}\prod_{a\in F} \Psi_a(\bold x_a,\bold y_a)   \tag{2.1}
$$
对任意的局部函数$F=\left\{\Psi_a  \right\}$，$\Psi_a : V^{|a|} \rightarrow \bold \Re^{+}$。（这些函数被称为因式函数或者是兼容性函数）。我们将偶尔用随机场来指特定的由无向图定义的分布。当我们讨论如何将一个图模型用因式分解来表示的时候，为什么用图模型这个名词就很明显了。

​	这个常数$Z$是正交化因子，用来确保分布$p$的和等于1，它的定义如下：
$$
Z = \sum_{\bold x,\bold y}\prod_{a\in F}\Psi_a(\bold x_a, \bold y_a)  \tag{2.2}
$$
$Z$有时被称为划分函数。要注意的是，$\bold x,\bold y$的可能取值是指数级别的个数，因此，通常计算$Z$是相当困难的，但是已有很多的方法来近似求解它。

​	我们通常进一步的假定每一个局部函数有这样的形式：
$$
\Psi_a(\bold x_a,\bold y_a) = \exp{\left\{\sum_k \theta_{ak}f_{ak}(\bold x_a,\bold y_a) \right\}}   \tag{2.3}其中，$\theta_a$是实值参数向量，$\left\{ f_{ak} \right\}$是特征函数或者充分统计量。如果$\bold x,\bold y$ 是离散的，那么这个计算不失一般性。因为我们可用指示函数来表示每一个可能的值，也就是说，如果我们有一个特征函数
$$
其中，$\theta_a$是实值参数向量，$\left\{ f_{ak} \right\}$是特征函数或者充分统计量。如果$\bold x,\bold y$ 是离散的，那么这个计算不失一般性。因为我们可用指示函数来表示每一个可能的值，也就是说，对于每可能的取值$\bold x^*_a,y^*_a$, 特征函数可以写成$f_{ak}(\bold x_a,\bold y_a)=\bold 1_{\left\{\bold x_a= \bold x^*_a\right\}}\bold 1_{\left\{\bold y_a= \bold y^*_a\right\}}$。

​	同时，这种参数化的结果用参数$\theta$ 表示的关于变量$V$的概率分布是一个指数族分布。事实上，在本教程中许多关于CRFs参数估计的讨论都应用了广义指数族分布中的方法。

​	就像我们已经提到的，一个图模型的因式分解和它定义域中变量的条件依赖是紧密相关的。这种关系可以通过无向图模型，也就是著名的markov network来理解。markov network 可以直接表示多变量分布的条件依赖性。假定 $G$是一个由变量集$V$的无向图，也就是说，$G$对每一个随机变量都对应有一个节点。对于一个变量变量 $s\in V $，$N(s)$表示$s$的相邻节点。那么我们说一个分布$p$关于$G$是markov network，如果它满足局部markov性：对任意的两个变量$s,t \in V$，变量$s$是关于它的邻居节点$N(s)$ 条件独立与$t$的。直觉上说就是，$s$的邻居节点包含了预测该值的所有信息。

​	给定一个分布$p$的因式分解，该分解形式像$(2.1)$式所表示，可以通过用线成对的连接出现在同一个局部函数中的随机变量，来构建一个等价的markov network。相对与这图来说，$p$是markov是相当明显的，因为遵循这$(2.1)$ 式的条件分布$p(x_s|\bold x_{N(s)})$ 是一个仅关于出现在markov 毯中变量的函数。换句话说，如果$p$根据图$G$因式分解，那么$p$就是关于图$G​$的markov。

​	只要$p$是严格正的，相反方向的图也是成立的。下面的经典的结果证实了了这一点：



​	但是从因式分解的观点来看，markov network 有一个不良的歧义性。考虑图$(2.1)$中左侧的三个节点的markov network，对于这个图而言，任何的能够因式分解$p(x_1,x_2,x_3)\propto f(x_1,x_2,x_3)$的（$f$是正的）分布都是markov。但是我们可能希望用一个更受限的参数化的形式：$p(x_1,x_2,x_3)\propto f(x_1,x_2)g(x_2,x_3)h(x_1,x_3)$ 来表示。这种形式的模型族更小，因此更容易进行参数估计。但是markov network 不能区分这两种参数化的形式。为了描述模型更加精确，$(2.1)$式的因式分解可以用因子图直接表示出来。

。。。。。



#####  2.1.2 有向图

​	鉴于无向图模型中的局部函数不需要一个直接的概率解释，有向图模型描述了一个分布是如何分解成局部条件概率分布的。假定$G=(V,E)$是一个有向无环图，其中$\pi(v)$是图$G$中节点$v$的父节点。一个有向无环图是可以分解为下面的分布族：
$$
p(\bold x, \bold y)=\prod_{v\in V}p(y_v|\bold y_{\pi(v)})   \tag{2.4}
$$
可以看到通过在图$G$上用结构归纳法，分布$p$是归一化的。有向图模型可以看成是一种因子图，其中单个因子是以一种特殊的形式局部归一化的，所以它的全局归一化因子$Z=1$。就像我们在$2.2.3$节中解释的那样，有向图经常被用做生成模型。有向图的一个例子是朴素贝叶斯模型，对应于在图$2.2$左侧的图。



#### 2.2 生成模型与判别模型的比较

​	在这节中我们讨论几个简单的图模型应用到自然语言处理中的例子。尽管这些例子都非常有名，但它们有助于澄清之前章节提到的定义，同时通过阐述一些在后面条件随机场中所讨论的方法。我们特别关注HMM，因为它与线性链CRF密切相关。



##### 2.2.1 分类

​	首先我们要讨论的是分类问题，即是给定特征向量$\bold x=(x_1,x_2,...,x_K)$ 去预测单个离散的分类值$y$。完成这个问题的简单方法是假定给定类标签的情况下，所有特征都是独立的，这样的分类器被称为朴素贝斯分类器。它是基于一个联合概率模型：
$$
p(y,\bold x)=p(y)\prod_{k=1}^K p(x_k|y)   \tag{2.5}
$$
这个模型由图$2.2$（左侧）的有向图给出。我们也可以将这个模型写成因子图的形式，只要令$\Psi(y)=p(y)$，对每一个特征，$\Psi_k(y,x_k)=p(x_k|y)$。图$2.2$（右侧）给出了因子图。

​	另一个著名的分类器是Logistic 回归（在NLP社区中有时候称为最大熵分类器），它也可以很自然的用图模型来表示。在统计上，这种分类器工作方式是假定每个类别的对数概率$\log p(y|\bold x)$是$\bold x$的线性函数加上一个正交化的常数。这就推导出了条件分布：
$$
p(y|\bold x)=\frac{1}{Z(\bold x)}\exp{\left\{ \theta_y+\sum_{j=1}^K\theta_{y,j}x_j \right\}}    \tag{2.6}
$$
其中$Z(\bold x)=\sum_y \exp{\left\{ \theta_y+\sum_{j=1}^K\theta_{y,j}x_j \right\}}$是正交化常数，$\theta_y$是一个偏置量，对应于朴素贝叶斯中的$\log p(y)$。这里不是对每一个类都用一个权重向量（像(2.6)中所示的那样），我们可以采用对所有类别共享一个权重集合的形式。这个技巧是用来定义仅对单个类别是非0的特征函数的集合。要做到这一点，特征函数可以这样定义：特征权重为$f_{y',j}(y,\bold x)=\bold 1_{y'=y}x_j$，偏置项是$f_{y'}(y,\bold x)=\bold 1_{y'=y}$。现在我们就可以用$f_k$来索引特征函数$f_{y',j}$，用$\theta_k$来索引相应的权重$\theta_{y',j}$ 。用这种表示技巧，Logistic回归模型变为：
$$
p(y|\bold x)=\frac{1}{Z(\bold x)}\exp{\left\{\sum_{k=1}^K\theta_k f_k(y,\bold x)\right\}}  \tag{2.7}
$$
我们引入这个符号是因为它呈现了我们之后将要讨论的条件随机场。



##### 2.2.2 序列化模型

​	分类预测仅仅是单个类别的预测，但图模型的真正能力在于能够对相互依赖的多变量类别进行建模预测。在本章中，我们讨论最简单的变量依赖形式，这些输出变量是序列化布置的。为了说明这个模型，我们讨论了自然语言处理中的一个应用：命名实体识别（NER）任务。命名实体识别是对文本中名字的识别和分类。包括地址，比如说中国；人物，比如乔治布什；组织，不如联合国。命名实体识别任务就是，将给定一个句子分割成实体的词语，并通过类型（人物，组织，地址等等）将实体分类。这个问题的挑战是许多命名实体太罕见，以至于在大的训练集中都没有出现，因此分类系统要识别他们仅能依靠上下文。

​	解决NER的一种方法是独立的为每一个单词分类，就是将单词分为人物、位置、组织、或者其他（意思是不是一个实体）其中一个。这个方法的问题是，它假定给定输入，所有的命名实体标签都是独立的。但事实上，相邻单词的命名实体标签死相互依赖的，例如，尽管纽约是个位置，但是纽约时报是个组织。放宽独立性假设的一种方法是将输出变量排列成一个线性链。这就是HMM。一个HMM为一个观测序列$X={\left\{ x_t \right\}}_{t=1}^T$建模，假定这个观测序列具有潜在的状态序列$Y={\left\{ y_t \right\}}_{t=1}^T$ ，该状态序列是从有限的状态$S$中获取的。在这个命名实体的例子中，每一个观测变量$x_t$代表在位置$t$的单词，每一个状态$y_t$代表命名实体的标签，也就是实体类型人物、位置、组织、和其他。

​	为了能够简单的对$p(\bold y,\bold y)$分布进行建模，HMM做了两个独立性假设。首先，它假定每一个状态仅依赖它前一个状态，也就是说，给定前一个状态$y_{t-1}$，状态$y_t$是独立于$y_1,y_2,...,y_{t-2}$的。第二，它还假定每一个观测变量$x_t$仅依赖当前状态$y_t$。即状态序列$\bold y$和观测序列$\bold x$的联合概率可以分解为：
$$
p(\bold y,\bold x)=\prod_{t=1}^Tp(y_t|y_{t-1})p(x_t|y_t)  \tag{2.8}
$$
其中，为了简化符号，我们将初始状态分布$p(y_1)$用$p(y_1|y_0)$来表示。在自然语言处理中，HMM用来解决序列化标注任务，例如词性标注、命名实体识别和信息提取。



##### 2.2.3 比较

​	在本节中描述的模型中，两个是生成模型（朴素贝叶斯和HMM）和一个判别模型（Logistic 线性回归模型）。通常，生成模型是对联合概率$p(y,\bold x)$进行建模，就像朴素贝叶斯的形式$p(\bold y)p(\bold x|\bold y)$。换句话说，他们描述了输出变量作为输入变量的函数是如何从概率角度产生的。另一方面，判别模型仅关注条件分布$p(y|\bold x)$。在这节中，我们讨论生成模型和判别模型的不同，以及判别模型的优势。为了能够具体一些，我们关注naive Bayes和Logistic regression这两个模型 ，但是本节中对于它们之间差异的讨论也适用于任意结构的生成模型和条件随机场。

​	





























































