# 最大似然函数、交叉熵和经验分布的关系

这是对《deep learning》书中(5.59)式和均方损失是经验分布和高斯模型之间的交叉熵的理解。

## 经验分布

何为经验分布？经验分布的随机变量可能取值就是训练集中所有出现的情况，那么不论原来的随机变量式离散的，还是连续的，经验分布的随机都是离散的，比如：学生身高是个连续的随机变量$X$，现在有样本$(x_1=1.67,x_2=1.89,x_3=1.7,x_4=1.7)$，那么经验分布是个离散变量$\hat X$，其可能的取值就是$(1.67,1.7,1.89)$。

* 最大似然函数能推导出交叉熵，他们俩本质是一回事，推导的桥梁就是经验分布。对于连续变量，cdf累积经验分布的定义：
  $$
  \begin{equation}
  \widehat p_{data}(x)=\left\{
  \begin{array}{rcl}
  0 & & {x < x_1}\\
  k/N & & {x_k \leq x < x_{k+1}}\\
  1 & & {x \geq x_{k+1}}
  \end{array} \right.\tag 1
  \end{equation}
  $$
  其中$x_k​$是已经重新按照大小排列过得，但对于连续变量，我们多用其概率密度函数，这需要用到$\bold{Dirac} \ delta​$函数$\delta(x)​$:
  $$
  \delta(x)=\left\{
  \begin{array}{rcl}
  +\infty && {x = 0}\\
  0 && {x_k \neq 0}
  \end{array} \right.
  $$
  并且$\delta(x)$满足以下概率密度函数的约束：
  $$
  \int_{-\infty}^{+\infty}\delta(x)dx=1  \tag 2
  $$
  $\delta(x)$被定义在除了0以外，所有点的值都为0，但其积分为1，显然这不是正常的积分函数，$\delta(x)$被称为广义函数，广义函数是依据积分性质定义的数学对象。

  依据[维基百科](https://en.wikipedia.org/wiki/Dirac_delta_function)，还有下面的性质：
  $$
  \int_{-\infty}^{+\infty}f(x)\delta(x)dx=f(0)  \tag 3
  $$
  现在来定义经验分布的概率密度：
  $$
  \hat p(x)=\frac 1 N \sum_{i=1}^N\delta(x-x_i)  \tag 4
  $$
  可以看出，对于训练集，经验分布$\hat p_{data}(x)$就是式$(1)$ ：
  $$
  \widehat p_{data}(x)=\int_{-\infty}^{x}\frac 1 N \sum_{j=1}^N \delta(x-x_j)dx=\left\{
  \begin{array}{rcl}
  0 & & {x < x_1}\\
  k/N & & {x_k \leq x < x_{k+1}}\\
  1 & & {x \geq x_{k+1}}
  \end{array} \right. \tag {4-1}
  $$
  同样$x_k$是训练集从小到大排列过的顺序。

  由此可以得到函数$f(x)$关于经验分布的期望：
  $$
  \begin{split}
  E_{x\sim \hat p_{data}}
  &=\int_{-\infty}^{+\infty} \hat p(x)f(x)dx\\
  &=\int_{-\infty}^{+\infty}\frac 1 N \sum_i^N \delta(x-x_i)f(x)dx\\
  &=\frac 1 N \sum_i^N \int_{-\infty}^{+\infty}\delta(x-x_i)f(x)dx\\
  &=\frac 1 N \sum_i^N f(x_i)
  \end{split} \tag {4-2}
  $$
  上式中第3步用到了式$(3)$。

  对于离散变量，就简单多了， $\hat p_{data}(X=x)=v(X=x)/N​$ .其中$v(X=x)​$表示$x​$出现的频数，$N​$是样本训练集总数。

  要说明的是：

  对于连续型变量，最大似然函数式概率密度的乘积，虽然没有用累积分布函数(cdf)，对应的是累积分布函数的乘积，所以其经验分布也要用累积分布函数；对于离散变量，最大似然函数用的是概率质量函数(pmf)，同样经验分布也就用pmf，所以用频率来表示概率分布就可以了，而不是cdf。

## 2关系推导

- 考虑到分类问题（离散型），最大似然估计：
  $$
  P(X;\theta)=\prod_kq^{N*p_k}(C_k;\theta) \tag5
  $$
  其中$p_k​$是第k类出现的频率，也就是经验分布，对似然估计取$log​$似然，再$\frac1 N​$，得到：
  $$
  L=\sum_k(p_k)logq(C_k,\theta) \tag {6}
  $$
  显然$(3)​$式就是交叉熵$H(p,q) = -\int{p(x)}\log{q(x)} dx=-\sum_k{p_k}{\log q_k}​$

* 考虑回归问题（连续型） ，给定一组含有$N​$个样本的$X=\{x_{(1)},...x_{(N)}\}​$，$x_i\in R​$，独立的由真是数据生成分布$P_{data}​$生成，$P_{model}​$是一族由$\theta​$为参数的概率密度函数$pdf​$，用来估计真实的概率密度函数$P_{data}​$，最大似然估计：
  $$
  \begin{equation}
  \begin{split}
  \theta_{ML}
  &=\arg\max\limits_{\theta}\prod_i^N{p_{model}(x_i;\theta)}\\
  &=\arg\max\limits_{\theta}\sum_i^N{\log p_{model}(x_i;\theta)}\\
  &=\arg\max\limits_{\theta}\frac 1 N\sum_i^N{\log p_{model}(x_i;\theta)}\\
  &=\arg\max\limits_{\theta}\frac 1 N \sum_i^N \int_{-\infty}^{+\infty}\delta(x-x_i)\log p_{model}(x;\theta)dx\\
  &= \arg\max\limits_{\theta}\int_{-\infty}^{+\infty}\frac 1 N \sum_i^N \delta(x-x_i)\log p_{model}(x;\theta)dx\\
  &=\arg\max\limits_{\theta}-H(\widehat p_{data},p_{model})\\
  &=\arg\min\limits_{\theta}H(\widehat p_{data}, p_{model})
  \end{split} \tag 7
  \end{equation}
  $$
  以上第3步对上式乘以$\frac {1}{N}$不影响结果。从此推导可以看出最大似然估计、交叉熵和经验分布的关系：

  **对于概率模型，最大似然估计是训练集上的经验分布和建立在模型上的概率分布之间的交叉熵。**

* 考虑这样一个问题：对于连续型问题，为何可以用概率密度来进行最大似然估计？
  $$
  \begin{align}
  P(\bold x;\theta)
  &=\int_{\Omega} p(x^{1},...x^{N}){dx^1...dx^N}\\
  &=\int_{\Omega} p(x^{1})...p(x^{N}){dx^1...dx^N}\\
  &=\int_{\Omega} \prod_{i=1}^{N} p(x^i){dx^1...dx^N}
  \end{align} \tag 8
  $$
  $x^i​$是$\bold x​$第$i​$个位置对应的元素变量，可见要使得$P(\bold x;\theta)​$最大化，只要使得积分量$\prod_{i=1}^{N}p(x^i)​$最大即可。

 ## 例子：均方损失

在此先要说一下，对于我们遇到的问题，都是输入和输出构成的样本，也就是这样的训练集$\left\{(\bold x_i,y_i),...,(\bold x_N,y_N) \right\}$，上面的分析如何用到这上面？用条件概率$p(y/\bold x;\theta)$来替代上面的$p(\bold x)$即可。

比如此时的经验分布
$$
\hat p(y|\bold x)=\frac 1 N \sum_{i=1}^N\delta(x-x_i)  \tag 4
$$
回到当前的讨论：均方损失是经验分布和模型是高斯分布之间的交叉熵，如何分析？

假设样本$iid$，那么条件对数似然乘以$\frac 1 N​$就是：
$$
-H(\widehat p_{data},p_{model})=E_{x\sim{\widehat p_{data}}}\log p(\bold y|\bold x;\theta)=\frac 1 N\sum_i^N\log p(y_i/\bold x_i;\theta)=-\frac 1 N(\sum_i^N \frac {\| \hat y_i-y_i\|^2} {2\sigma}-m\log\sigma-\frac m 2 \log(2\pi)) \tag9
$$
均方损失：
$$
MSE_{train}=\frac 1 N \sum_i^N \|\hat y_i-y_i\|^2
$$
可以看出两者是等价的。