# 矩阵求导
机器学习中因为要使用梯度下降法，深度学习中，要使用BP算法对权重矩阵进行更新，都会涉及到矩阵求导，虽然矩阵求导算法已经包含在各种包之内，可以直接调用，但是理解矩阵求导的的过程，对于理解如何进行学习还是很有帮助的
## 维度相容原理
严格的矩阵求导涉及到矩阵微分，可参考张贤达的《矩阵分析》。为了能够快速推导出对矩阵的导数，有一套实用性的技巧，叫维度相容原理。  
先要说明一下，矩阵求导有两种布局结构，一种是分子布局，一种是分母布局，采用哪一种都可以，只要保持前后布局的一致性就行了。我们用维度相容原理，使用的是分母布局，具体原因看过后面的分析就会明白。  
还有一点就是，损失函数的表达存在两种形式，一个是矩阵的表达，一个是对单样本损失函数的求和表达，一般在推导损失函数的时候，使用后者更为方便，有时候只能写成后者的形式，但是在编程表达梯度的时候，一般都进行向量化处理。  
在机器学习中，以最常见的线性回归的例子来说明上面两点，并使用维度相容来求导。  
$$
J = \sum_{i=1}^m (w^Tx_i-y_i)^2 = (Xw-y)^T(Xw-y)   \tag{1}
$$
如果使用随机梯度下降法，那么使用求和的式子更方便一些，此时：
$$
\frac{\partial J_i}{\partial w} = 2(w^Tx_i-y_i)x_i   \tag{2}
$$
上式求导先按照实数求导来做，然后再进行调整。因为采用分母布局，我们很容易知道上式左边的维度是$R^{n\times 1}$，根据维度相容原理，那么右边的维度也必须是$R^{n\times 1}$，现在我们看右边相乘后是不是满足维度相容，不满足就进行调整。$x_i \in R^{n\times 1}, \ \ (w^Tx_i-y_i) \in R^{1}$，相乘之后，可见维度是满足的，不需要更改。  
当我们不用随机梯度下降法，采用批量梯度下降，或者因为线性回归存在闭式解，最优解可以直接求出，那么就会使用后一种矩阵的表达。另外，在深度学习中，很少用随机梯度下降，一般会使用小批量随机梯度下降。同样先按照实数求导法则，然后按照梯度相容原则进行调整：
$$
\frac{\partial J}{\partial w} = 2(Xw-y)X   \tag{3}
$$
左边的维度是$R^{n\times 1}$，右边$X \in R^{m\times n}, \ \ (Xw-y)\in R^{m\times 1}$，那么为了维度的一致性，则需要将上式调整为：
$$
\frac{\partial J}{\partial w} = 2X^T(Xw-y)   \tag{4}
$$
在深度学习中，使用前馈网络来说明一下，这里参考了[知乎上的这篇文章](https://zhuanlan.zhihu.com/p/25202034)。  
假设某一层的$score = XW+b$，其中$X\in R^{m\times n}, \ \ W\in R^{n\times c}, \ \ b\in R^{1\times c}, \ \ score \in R^{m\times c}$，这里要说明一下，为什么$b \in R^{1\times c}$，b是偏置量，它对预测出的每一个类别得分进行偏置，而$XW \in R^{m\times c}$的第i个行向量就是第i个样本预测出各个类别的得分，所以b必须是一个行向量，这样相加才能对每一个类别得分进行偏置，这里要注意的是，这两个矩阵相加维度不一致，这在numpy中使用了广播的原则，也就是会将b的行向量复制m份，构造出一个$m\times c$的矩阵，矩阵的每一行都是b。  
现在假设我们上层已经告诉你$\frac{dL}{dscore}$，让求$\frac{dL}{dW}, \ \ \frac{dL}{db}$，现在用维度相容原则来求。  
$$
\frac{dL}{dW} = \frac{dL}{dscore} \frac{dscore}{dW}   \tag{5}
$$
上式左边的维度是$R^{n\times c}$，右边第一项的维度是$R^{m\times c}$，为了使得维度一致，那么最后一项就是$R^{n\times m}$，并且上式右边顺序写反了。而最后一项的导数我们用实数求导来求，结果是$X$，但为了使得维度是$R^{n\times m}$，需要进行转置处理，那么最终的结果是：
$$
\frac{dL}{dW} =X^T \frac{dL}{dscore}    \tag{6}
$$
现在再来看看$\frac{dL}{db}$，其维度是$R^{1\times c}$：
$$
\frac{dL}{db} = \frac{dL}{dscore} \frac{dscore}{db}   \tag{7}
$$
通过维度相容分析，最后一项是$R^{1\times n}$，最后一项按照实数求导原则就是1，那么其结果就是元素全是1，维度是$R^{1\times n}$的向量，最终结果是：
$$
\frac{dL}{db} = I_{1\times n}\frac{dL}{dscore}   \tag{8}
$$
## 矩阵求导中的逐元素运算
矩阵求导中经常涉及到逐元素的问题，其中两个关键的函数逐元素操作这里进行说明:
$$
e^{x}=\left[
\begin{matrix}
e^{x_1}\\
\vdots   \\
        e^{x_n}
\end{matrix}
\right] \ \ \ \ 
\ln x=\left[
\begin{matrix}
\ln x_1 \\
        \vdots \\
        \ln x_n
\end{matrix}
\right]
$$
那么对于自身的求导：
$$
\frac{\partial e^{x}}{\partial x}=\left[
\begin{matrix}
e^{x_1}\\
\vdots   \\
        e^{x_n}
\end{matrix}
\right] \ \ \ \ 
\frac{\partial \ln x}{\partial x}=\left[
\begin{matrix}
\frac{1}{x_1} \\
        \vdots \\
        \frac{1}{x_n}
\end{matrix}
\right]
$$
对于矩阵也是一样的操作，是逐元素操作，这两个关键的东西要记住，因为在神经网络的输出层中的softmax中构造交叉熵损失的时候BP求导会用到。  
另外的逐元素操作涉及到矩阵元素求和问题，比如对于矩阵$A \in R^{m\times n}$，假设每一行(或者每列)任意取一个元素求和为$l$，假设第$i$行取第$y_i$位置处的元素，则$l = \sum_i^m A_{i,y_i}$。那么$\frac{\partial l}{\partial A}$如何求？  
$$
\left[
    \begin{matrix}
    a_{11} & \dots & a_{1n}\\
        \vdots & \vdots & \vdots \\
        a_{m1} & \dots & a_{mn}
    \end{matrix}
\right] \odot
\left[
    \begin{matrix}
    0   & \dots & 1_{y_1}& \dots & 0\\
        \vdots & \vdots & \  & \vdots \\
0   & \dots & 1_{y_m}& \dots & 0
    \end{matrix}
\right]
$$
上面就是对应元素相乘然后放在对应的位置，结果还是$R^{m\times n}$的矩阵，但是这样可以从矩阵$A$中提取要求和的元素，然后将结果矩阵逐元素求和就可以得到$l$。我们假设后一个提取矩阵为$B$。
我们先给出求导结果：
$$
\frac{\partial l}{\partial A} = 
\left[
    \begin{matrix}
    0   & \dots & 1_{y_1}& \dots & 0\\
        \vdots & \vdots & \vdots & \vdots \\
0   & \dots & 1_{y_m}& \dots & 0
    \end{matrix}
\right]
$$
怎么得到的呢？我是凭着直觉，下面简单证明一下.
$$
\Delta l = sum(\Delta A \odot B)
$$
矩阵$A$在提取位置增加多少，$l$就线性增加多少，$l$是矩阵$A$在提取位置的线性函数，线性系数是1，而根据求导法则，$\frac{\partial l}{\partial A}$和$A$维度相同，所以导数就是$B$。  
最后来看一下，如果$l_{m\times 1}$是矩阵$A_{m\times n}$每一行(或者每一列)的元素之和组成的向量，那么$\frac{\partial l}{\partial A}$怎么求？我们可以使用维度相容原理来解决，假设$L$是$l$元素的和，那么$L$也就是$A$所有元素的和，则有：
$$
\frac{\partial L}{\partial A}=\frac{\partial L}{\partial l}\frac{\partial l}{\partial A}
$$
左边根据上面谈到可以知道，其结果是和$A$同样大小的所有元素为1的矩阵，右边第一项是大小为$R^{m\times 1}$，元素全为1的向量，那么最后一项，根据维度相容，其结果可能是$R^{1\times}$，或者是$R^{m\times n}$，前者依据的是矩阵乘法，后者依据的是，对第一项进行广播后，然后逐元素相乘最后一项。最后一项是向量对矩阵求导，不可能是一个向量，必定是一个矩阵，所以只可能是后一种，由于两者逐元素相乘后，结果是等于左边的所有元素为1的矩阵，又$l$是每一行元素线性之和，其线性权重为1，所以最后一项的求导结果就呼之欲出了，就是和左边导数一样。
有这3个原则，我们就可以求出输出层上交叉熵损失对权重的求导。  
假设输出层$score = XW+b$，其中$X\in R^{m\times n}, \ \ W\in R^{n\times c}, \ \ b\in R^{1\times c}, \ \ score \in R^{m\times c}$，对应的交叉熵很难用矩阵表达出来，那么就先求出第$i$个样本的损失$l_i$，它是$score$第$i$行的softmax的交叉熵，假设第$i$行是$s_i$：
$$
L  = \sum_{i=1}^{m} l_i = \sum_{i=1}^m -\log \frac{e^{s_{i,y_i}}}{\sum_{j=1}^c e^{s_{i,j}}} = -\sum_{i=1}^m s_{i,y_i} + \sum_{i=1}^m \log \sum_{j=1}^c e^{s_{i,j}}  \tag{1}
$$
要求$\frac{\partial L}{\partial W}$和$\frac{\partial L}{\partial b}$，直接求导是个不合适的做法，BP的关键就在于**链式法则，一步步求导**：
$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial score} \frac{\partial score}{\partial W} \\ 
   \ \\
\frac{\partial L}{\partial b} = \frac{\partial L}{\partial score} \frac{\partial score}{\partial b}
$$
我们只要求出$\frac{\partial L}{\partial score}$，那么后面根据维度相容原理就很容易得到了。  
我们将$(1)$式分为前后两个部分来求，第一部分$L_1 = -\sum_{i=1}^m s_{i,y_i}$就是我们的第二个原则，就是提取$score$矩阵相应位置处的元素然后进行求和，所以：
$$
\frac{\partial L_1}{\partial score} = \left[
    \begin{matrix}
    0   & \dots & -1_{y_1}& \dots & 0\\
        \vdots & \vdots & \ & \vdots \\
0   & \dots & -1_{y_m}& \dots & 0
    \end{matrix}
\right]  \tag{2}
$$
第2部分$L_2 = \sum_{i=1}^m \log \sum_{j=1}^c e^{s_{i,j}}$，首先它是对向量$q = [\log \sum_{j=1}^c e^{s_{1,j}},\dots, \log \sum_{j=1}^c e^{s_{m,j}}]^T$的所有元素求和，也是符合第二原则的，那么：
$$
\frac{\partial L_2}{\partial q} = [1,\dots,1]_{m\times 1}^T  \tag{3}
$$
而$q = \log p, p = [\sum_{j=1}^c e^{s_{1,j}},\dots, \sum_{j=1}^c e^{s_{1,j}}]^T$，显然符合第一原则，那么：
$$
\frac{\partial q}{\partial p} = [\frac{1}{p_1},\dots,\frac{1}{p_m}]^T   \tag{4}
$$
而p矩阵$e^{score}$的每行元素之和构成的向量，正好符合第3个原则：
$$
\frac{\partial p}{\partial score}=\frac{\partial p}{\partial e^{score}}\frac{\partial e^{score}}{\partial score}=I_{m\times c}\odot e^{score}   \tag{5}
$$
上式中间第二项使用了第一原则，将$(3)、(4)、（5)$相应元素广播后在逐元素相乘，就得到了结果:
$$
\frac{\partial L_2}{\partial score}= [\frac{1}{p_1},\dots,\frac{1}{p_m}]^T \odot e^{score} = \left[
\begin{matrix}
\frac{e^{s_{11}}}{\sum_j^c e^{s_{1,j}}} & \dots & \frac{e^{s_{1c}}}{\sum_j^c e^{s_{1,j}}} \\
    \vdots & \vdots & \vdots \\
        \frac{e^{s_{m1}}}{\sum_j^c e^{s_{m,j}}} & \dots & \frac{e^{s_{mc}}}{\sum_j^c e^{s_{m,j}}} \\
\end{matrix}
\right]
$$
表达式为了简洁，用$s$代替了$score$，明眼的你一定可以看到，$L_2$对$score$的导就是对$score$进行$softmax$激活后的结果。  
将两部分求导求和，就是$\frac{\partial L}{\partial score}$的结果，很容易看出来，最终结果就是对score进行softmax后再在与训练集标签相同位置元素减去1。  
通过维度相容分析，那么最终损失对权重的导数：
$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial score} \frac{\partial score}{\partial W}=X^T\frac{\partial L}{\partial score}
$$
## 神经网络中的BP传播
上面我们求出了损失函数对输出层权重的梯度，那么如何对输出层前一层的权重进行更新呢？  
我们以前馈神经网络为例，先对符号标记进行说明：$W^l, \ b^l, \ f^l(\centerdot), \ a^l$分别是第$l$层的权重、偏置量、激活函数和激活值，向前传播有：
$$
a^l = f^l(W^la^{(l-1)} + b^l)  \tag{6}
$$
要说明的是输入层为第0层，输出层经过激活函数激活后(比如softmax)，还需要在此基础上，构造损失函数。  
我们来推导一下导数向后传播，相邻层之间的关系，假设已经知道损失函数$L$对第$l$层激活值的导数，那么它与$L$对$l-1$层激活值导数是什么关系呢？
$$
\frac{\partial L}{\partial a^{(l-1)}} = \frac{\partial L}{\partial a^{(l)}} \frac{\partial a^l}{\partial a^{(l-1)}}  = \frac{\partial L}{\partial a^{(l)}} \frac{\partial f^l(\centerdot)}{\partial a^{(l-1)}}    \tag{7}
$$
由此，我们可以把第l层损失函数对激活值的导数保存下来，在向后传播的时候可以使用。但是我们要对权重求导，对激活之求导只是中间结果:
$$
\frac{\partial L}{\partial w^{(l-1)}} = \frac{\partial L}{\partial a^{(l-1)}} \frac{\partial a^{(l-1)}}{\partial W^{(l-1)}} \tag{8}
$$
其中$\frac{\partial L}{\partial a^{(l-1)}}$已经通过$(7)$式求得。  
我们如何能跟上一节中求导进行匹配呢？重新考虑上一节中求导过程，我们通过链式法则，求中间量的导数$\frac{\partial L}{\partial score}$，假设输出层是第$l$，而$score=W^la^{(l-1)}+b^l$，可见中间量的导数并不是对激活值的导数，(上一节是对所有训练集或者小批量样本集进行梯度更新的，现在我们假设上一节处理的时单个样本，这样才好和神经网络中网络图做比较，因为神经网络图是当个样本的过程)。那么交叉熵损失$L$对$a^l$(也就是经过softmax后的score)的导入如何求呢？  
$$
a^l = softmax(score) = [\frac{e^{s_1}}{\sum_j^c e^{s_j}},\dots,\frac{e^{s_c}}{\sum_j^c e^{s_j}}]  \\
L = -\log a_{y_i}^l \\
\frac{\partial L}{\partial a^l}=-\frac{1}{a_{y_i}}[o,...,1_{y_i},...,0] \tag{9}
$$
现在还有一个问题，那就是上一节对第$l$层权重求导的时候，为什么我们没有考虑要求$\frac{\partial L}{\partial a^l}$呢？这有两个原因，其中一个是最后一层后面没有层了，无法借助后面一层来求它，要求也是像$(9)$式那样直接求；另一个原因，在最后我们会提到。但是为了BP，还是由必要求的。  

现在顺着BP传播的思路，我们再来考虑一下对输出层权重求导。根据公式$(8)$：
$$
\frac{\partial L}{\partial W^l} = \frac{\partial L}{\partial a^l} \frac{\partial a^l}{\partial W^l} \tag{10}
$$
右边第一部分已经求出，现在看看如何求出第二部分，同样使用链式法则：
$$
\frac{\partial a^l}{\partial W^l} = \frac{\partial a^l}{\partial score}\frac{\partial score}{\partial W^l} \tag{11}
$$
上式右边第二项根据维度相容原理是很容易求得的，关键是右边第一项的求导。  
$$
\frac{\partial a^l}{\partial score} = ?
$$
发现很难求，这是因为softmax比较复杂，而我们构造损失函数的时候，因为是交叉熵损失，能够消弭softmax造成的复杂性，交叉熵损失公式中，已经大大简化，甚至看不出softmax的影子，所以在第二章，能够轻松的求解，而这里，把softmax保留下来，求导就麻烦了，这也是上面说的第二个原因，要求输出层对权重的导数是没有必要求对激活值的求导数的，但是为了前一层权重求导，还是有必要求得。幸好只是在最后一层使用softmax激活函数，前面层中的激活函数例如RELU等，都很简单，$\frac{\partial a^l}{\partial score}$求导比较容易。
