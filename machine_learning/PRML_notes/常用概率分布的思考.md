## 常用概率分布的思考(上)

### 对于二元变量

 #### 经常用到bernoulli分布，二项式分布，beta分布

* bernoulli 分布

  $x$取值是二元的$\left\{0,1\right\}$，假设
  $$
  p(x=1/ \mu)=\mu \tag 1
  $$
  那么$x\sim bern(x/\mu)$:
  $$
  bern(x/\mu)=\mu^x(1-\mu)^{1-x} \tag 2
  $$
  从而：
  $$
  \begin{aligned}
  E(x)=\mu \\ 
  var(x)=\mu(1-\mu)
  \end{aligned} \tag 3
  $$
  极大似然估计：

  ​	对于训练集$D=\left\{x_1,...,x_N \right\}​$ ，假设观察值独立同分布，那么对数似然函数为：
  $$
  LL=\log p(D/ \mu)=\log \prod_{i=1}^{N}\mu^{x_i}(1-\mu)^{1-{x_i}}=\sum_{i=1}^N(x_i\log \mu+(1-x_i)log(1-\mu)) \tag 4
  $$
  最大化$LL$，可得：
  $$
  \mu_{ML}=\frac {1}{N} \sum_{i=1}^N x_i \tag 5
  $$
  说明：假设数据集$D$是$N$次抛硬币正反面的样本，如果$N$次皆抛正面，那么极大似然估计抛正面的概率是1，显然是不合理的，这说明，极大似然估计容易过拟合，即过于遵从样本的数据特征。

* 二项分布

  当做$N$次试验，$m$次为正，以$m$为变量，那么其概率分布：
  $$
  p(m)=C_N^m\mu^m(1-\mu)^{N-m}=\frac {N!}{(N-m)!m!}\mu^m(1-\mu)^{N-m}   \tag 6
  $$
  对于二巷分布的期望$E(m)$和$Var(m)$的求法这里要说明一下：

  当求期望$E(m)$和$Var(m)$，若按照对应的公式求，发现是很困难的，比如：
  $$
  E(m)=\sum_{m=0}^N mp(m) \\
  Var(m)=E[m^2]-E^2[m]
  $$
  显然是没法下手的，但通过把$m$看成是多个独立变量和时，通过下面的性质可轻松求出来：
  $$
  E(\sum_{i=1}^Nx_i)=\sum_{i=1}^NE(x_i)  \\
  Var(\sum_{i=1}^Nx_i)=\sum_{i=1}^NVar(x_i) \tag 7
  $$
  对于期望公式很容易证明，现在证明方差公式：
  $$
  \begin{aligned}
  Var(\sum_{i=1}^Nx_i)
  &=E[\sum_{i=1}^Nx_i-E(\sum_{i=1}^Nx_i)]^2 \\
  &=E[\sum_{i=1}^Nx_i-\sum_{i=1}^NE(x_i)]^2 \\
  &=E[\sum_{i=1}^N(x_i-E(x_i))]^2 \\
  &=E[\sum_{i=1}^N\sum_{j=1}^N(x_i-E(x_i))(x_j-E(x_j))] \\
  &=\sum_{i=1}^N\sum_{j=1}^NE[(x_i-E(x_i))(x_j-E(x_j))]\\
  &=\sum_{i=1}^N E[(x_i-E(x_i))^2] \\
  &=\sum_{i=1}^N Var(x_i)
  \end{aligned} \tag 8
  $$
  说明一下，上式第5步到第6步，当$i \neq j$时，$E[(x_i-E(x_i))(x_j-E(x_j))]=E[x_i-E(x_i)]E[x_j-E(x_j)]=0$。

  假设正面为1，反面为0，那么有：
  $$
  m=\sum_{i=1}^N x_i
  $$
  因为每一次抛都是独立的，则有：
  $$
  E(m)=E(\sum_{i=1}^Nx_i)=\sum_{i=1}^NE(x_i)=N\mu \\
  Var(m)=Var(\sum_{i=1}^Nx_i)=\sum_{i=1}^N Var(x_i)=N\mu (1-\mu)   \tag{9}
  $$

* beta分布

  介绍beta分布之前，先要说一下$gamma$函数：
  $$
  \Gamma(x)=\int_0^{+\infty}u^{x-1}e^{-u}du   \tag{10}
  $$
  $\Gamma(x)​$具体形式可能记不住，但是它的性质很有意思，很有用：
  $$
  \Gamma(x+1)=x \Gamma(x)   \tag{11}
  $$
  上式通过分布积分可以很容易证明。

  由于$\Gamma(1)=1$，故当$x$取整数的时候：
  $$
  \Gamma(x+1)=x!   \tag{12}
  $$
  也就说，阶乘是$\Gamma(x)​$函数在自变量取整数时的值。

  现在来说beta分布，beta分布产生源于共轭分布，当我们用最大后验估计来估计参数时，要计算后验分布，当然用最大后验估计是不需要完全计算出后验估计的（因为后验估计正比与先验分布和似然函数的乘积）。应该说是当用bayesian估计时，要计算后验概率，但是我们知道，计算后验概率不容易。但当我们选择合适的先验概率，使得后验概率和先验概率是同类型的分布，那么只要再进行归一化就可以求出后验概率了。

  对于训练集$D=\left\{x_1,...,x_N \right\}$ ，假设观察值独立同分布，那么似然函数为：
  $$
  p(D/\mu)=\prod_{i=1}^{N}\mu^{x_i}(1-\mu)^{1-x_i}=\mu^m(1-\mu)^{N-m}  \tag {13}
  $$
  其中$m$是训练集D中$x=1$的个数。

  后验概率：
  $$
  p(\mu/D) \propto p(D/\mu)p(\mu)   \tag{14}
  $$
  结合式$(13)$，当我们选择先验概率$p(\mu)$满足：
  $$
  p(\mu) \propto \mu^a(1-\mu)^b  \tag {15}
  $$
  那么后验概率就有：
  $$
  p(\mu/D) \propto \mu^{m+a}(1-\mu)^{N-m+b}  \tag{16}
  $$
  此时只要对其归一化(Normalization)，就可以求出$p(\mu/D)$。

  首先要对式$(15)$进行归一化，使得成为先验概率$p(\mu)$的概率分布，这就引出来beta分布：
  $$
  p(\mu)=\frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}   \tag{17}
  $$
  这里$a,b$可以为整数，也可以为小数，但上面讲过，对于$\Gamma(x)$函数，当$x$为整数时，我们有：$\Gamma(n+1)=n!$，所以在推导过程中，不妨将$a,b​$看出整数，则：
  $$
  p(\mu)=\frac {\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}=\frac {(a+b-1)!}{(a-1)!(b-1)!}\mu^{a-1}(1-\mu)^{b-1}   \tag{18}
  $$
  我们来证明$p(\mu)$作为一个概率分布所需要满足的条件：$\int_0^1p(\mu)d\mu=1​$
  $$
  \begin{aligned}
  \int_0^1p(\mu)d\mu
  &=\int_0^1 \frac {(a+b-1)!}{(a-1)!(b-1)!}\mu^{a-1}(1-\mu)^{b-1}d\mu  \\
  &=\frac {(a+b-1)!}{(a-1)!(b-1)!} \int_0^1 \mu^{a-1}(1-\mu)^{b-1}d\mu
  \end{aligned} \tag{19}
  $$
  我们来求上式的积分：
  $$
  \begin{aligned}
  \int_0^1 \mu^{a-1}(1-\mu)^{b-1}d\mu
  &=\int_0^1 (1-\mu)^{b-1}d\mu^a \\
  &=\frac {(b-1)}{a} \int_0^1 \mu^a(1-\mu)^{b-2}d\mu \\
  &=\frac {(b-1)!}{a(a+1)...(a+b-2)} \int_0^1 \mu^{a+b-2}d\mu \\
  &=\frac {(b-1)!}{a(a+1)...(a+b-1)}
  \end{aligned} \tag{20}
  $$
  上式第2步到第3步不断用到分部积分，代入到$(19)$式得证。

  这里给出beta分布的期望，同样可以通过分部积分求出，下面推导会用到：
  $$
  E(\mu)=\int_0^1 \mu p(\mu)d\mu=\frac {a}{a+b} \tag{20-1}
  $$
  这是很符合直觉的，样本为正的先验概率就是假定为：正的个数/总的个数

  现在考察后验概率分布式$p(\mu/D)$:
  $$
  p(\mu/D) \propto p(D/\mu)p(\mu) \propto \mu^{a+m-1}(1-\mu)^{b+l-1}   \tag{21}
  $$
  其中$l=N-m$，是样本取值为0的个数。

  就可以用beta函数来归一化后验概率：
  $$
  p(\mu/D)=\frac {\Gamma(m+l+a+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{a+m-1}(1-\mu)^{b+l-1}  \tag{22}
  $$
  现在通过最大后验估计来估计参数$\mu$，上式对$\mu$进行求导，得出最大后验估计：
  $$
  \mu_{MAP}=\frac {a+m-1} {m+l+a+b-2}=\frac {a+m-1} {N+a+b-2}   \tag{23}
  $$
  此时，即使样本取值都为0，即$m=N$，最大后验估计出的$\mu$也不为0，这样就在一定程度上避免了过拟合。

  当采用bayesian估计时：
  $$
  p(x=1/D)=\int_0^1p(x=1/\mu)p(\mu/D)d\mu=\int_0^1\mu p(\mu/D)d\mu=E[\mu/D]  \tag{24}
  $$
  从上式可以看出，结果是关于beta分布的期望， 参照式$(20-1)$得：
  $$
  p(x=1/D)=\frac {a+m} {m+l+a+b}  \tag{25}
  $$
  同样可以在一定程度上避免过拟合，并期可解释性更强。