## 基函数与核函数

+ 基函数

  在构建线性模型时，通过基函数对特征进行非线性变换，使得线性模型具有表达非线性的能力。基函数有径向基函数和$sigmoidal$基函数，傅里叶基函数。

  径向基函数：

  当特征变量$x$是一维，也就标量时：
  $$
  \phi_j(x)=\exp(-\frac{(x-\mu_j)^2}{2s^2})  \tag 1
  $$
  当$\bold x​$是向量时：
  $$
  \phi_j(\bold x)=\exp(-\frac{(\bold x-\boldsymbol\mu_j)^T(\bold x-\boldsymbol\mu_j)}{2s^2})  \tag {1.1}
  $$
  

  $sigmoid$基函数：

  当是标量时：
  $$
  \phi_j=\sigma(\frac{x-\mu_j}{s})  \tag2
  $$
  当是向量时：
  $$
  \phi_j(\bold x)=\sigma(\frac{(\bold x-\boldsymbol\mu_j)^T(\bold x-\boldsymbol\mu_j)}{2s^2})  \tag {2.1}
  $$

+ 要强调一点就是当目标变量$y$是一维的时候，我们采用上面的一套基函数，$y=\bold w^T \phi(\bold x)$，这没问题。但是当目标变量$\bold y$ 是向量时，那么我们是否需要给目标变量的每一个分量都分配一套基函数呢？比如，$y_1$用径向基函数（对应于式$(1.1)$），$y_2$用$sigmoid$基函数，$y_3$用其他基函数，或者还是用径向基函数，只是$s$不同了。用该怎么做呢？

  通常的做法是，采用同一套基函数，比如对$\bold y$的每一个分量都用一套径向基函数（$s$也是相同的）：
  $$
  \bold y=\bold W^T\phi(\bold x) \tag 3
  $$
  这样可以解耦，就是多变量回归可以分解为多个单变量回归。之前在《常用概率分布的思考(下)》中谈到过。

+ 固定基函数局限性

  由于我们假设的基函数在数据被观察之前就已经定了，当输入空间$\bold x$的维度很大时，基函数个数一般要比输入空间维度多得多，这就造成了维度灾难问题。还有超参数$\mu_j,s$的确定也不容易。

  因为限制的存在，我们才需要寻找更复杂的模型，比如支持向量机和神经网络等。

  虽然有维度灾难问题，但是真是数据的两个性质使得问题得以缓解。一是：输入空间向量$\bold x$通常分布在一个非线性流形空间内，由于输入向量之间较强的相互关系，使得有效维度比输入空间维度要小。如果我们采用局部基函数，那么我们就可以让基函数只分布在输入空间中只包含数据的区域。这种方法被用在径向基函数网络中,也被用在支持向量机和相关向量机当中。神经网络模型使用可调节的基函数,这些基函数有着 sigmoid 非线性的性质。神经网络可以通过调节参数,使得在输入空间的区域中基函数会按照数据流形发生变化。第二,目标变量可能只依赖于数据流形中的少量可能的方向。利用这个性质,神经网络可以通过选择输入空间中基函数产生响应的方向。

  注：上面的这一段需要后续学习和思考后进一步取理解。

