

## 后验概率正比于某些项的理解(PRML)

* 对于有参模型，参数$w$通常是连续变量，则有：

  **参数$w$的后验概率正比于目标变量的似然函数与$w​$的先验概率**

  对于训练集$T=\left\{(x_1,y_1),...,(x_N,y_N) \right\}$，其中$x_i \in R^m$，$y_i \in R​$。

  假设目标变量是独立同分布的，则似然函数是：
  $$
  p(\bold y/ \bold x,w)=\prod_i^N p(y_i/x_i, w) \tag 1
  $$
  说明：这里的$\bold y$是目标变量的集合($\bold y=(y_1,...y_N)$)，$ \bold x$是特征的集合$\bold x=(x_1,...,x_N)$。

  假设$w$的先验分布$p(w)$已知，那么$w$的后验分布：
  $$
  p(w/ \bold y,\bold x)=\frac {p(\bold y / \bold x, w)p(w)} {\int p(\bold y/ \bold x)p(w)dw}=\frac {p(\bold y / \bold x, w)p(w)} {E_w(p(\bold y/ \bold x))}=\frac {p(\bold y / \bold x, w)p(w)} {p(\bold y / \bold x)} \tag 2
  $$
  $p(w/ \bold y, \bold x)$是关于$w$的函数，因为$\bold y, \bold x$是已知训练数据，模型中仅有参数$w$未知。从上式可以看出，分母中$w$已经被积分积掉了，分母是一个与$w$无关的量。所以：
  $$
  p(w/ \bold y,\bold x) \propto p(\bold y / \bold x, w)p(w) \tag 3
  $$

* 对于无参模型，通常是分类问题，即目标变量是离散的：

  这在朴素贝叶斯中会遇到，对于训练集$T=\left\{(x_1,y_1),...,(x_N,y_N) \right\}​$，其中$x_i \in R^m​$，$y_i \in (C_1,...,C_K)​$。

  假设目标变量是条件独立，则有：
  $$
  p(C_i/ x)= \frac {p( x/C_i)p(C_i)} {p( x)}=\frac {\prod_j^m p(x^{(j)}/C_i)p(C_i)} {\sum_{k=1}^K \prod_j^m p(x^{(j)}/C_k)p(C_k)} \tag 4
  $$
  对于$p(C_i/  x)$，$ x$是已知数据，变量是$C_i$，分母与$C_i$无关，即给定$x$，它对任意的$C_i$，该分母都是相同的。

  所以：
  $$
  p(C_i/ x) \propto p(x/C_i)p(C_i) \tag 5
  $$
  这里要说明两点：

  1. 式$(4)$中的$x$并不是训练集中的，就是给定的一个新的特征，看它分类到$C_i$上的概率，可以直观的看出，应该把$x$分到使得$p(C_i/x)$最大的$C_i$类中。

  2. 要求$\max p(C_i/x)$，可以间接地求$\max \left\{p(x/C_i)P(C_i)\right\}$，那么就要算出$p(C_i)$和$p(x/C_i)$，求$p(x/C_i)$就要求$x$的每一个元素$x^{(j)}$的条件概率$p(x^{(j)}/C)$，这可以从极大似然估计中求得：

     假设$p(y=C_k)=\mu_k​$，则：
     $$
     p(y)=\prod_k^K {\mu_k}^{I(y=C_k)}  \tag 6
     $$
     则对数似然函数：
     $$
     LL=\sum_i^N \log p(y_i)=\sum_i^N\log \prod_k^K{\mu_k}^{I(y_i=C_k)}=\sum_i^N\sum_k^K I(y_i=C_k)\log \mu_k \\
     s.t. \ \sum_k^K\mu_k=1 \tag7
     $$
     其中$I(y=C_k)​$是示性函数，当$y=C_k​$时，结果为1，否则为0。

     利用$(7)$式进行最大似然估计，可得：
     $$
     p(y=C_k)=\frac {\sum_i^N I(y_i=C_k)} {N}  \tag 8
     $$
     现在求$p(x/C)​$，设第$j​$个特征$x^j​$可能的取值集合为$\left\{a_{j1},...,a_{jS_j}\right\}​$，条件概率$p(x_j=a_{jl}/y=C_k)​$的极大似然估计是：
     $$
     p(x_j=a_{jl}/y=C_k)=\frac {\sum_i^N I(x_i^j=a_{jl},y_i=C_k)} {N}  \\
     j=1,2,...,n;  \ l=1,2,...,S_j;  \ k=1,2,...,K   \tag9
     $$
     注意：

     ​	此处特征$x$的每个元素是离散的，这在判断是不是垃圾邮件经常用的模型，但若$x$的元素全是连续变量，或者有连续变量有离散变量，又该如何解决呢？

  3. 特别强调：式$(4)$用到了条件独立。何为条件独立？为什么要用？

     条件独立：
     $$
     p(a,b/c)=p(a/c)p(b/c) \tag {10}
     $$
     则说$a,b​$关于$c​$条件独立。

     可以通过以下推导：
     $$
     p(a,b/c)=\frac {p(a,b,c)}{p(c)}=\frac {p(c)p(a/c)p(b/ac)} {p(c)}=p(a/c)p(b/ac) \tag {11}
     $$
     可见$a,b$关于$c$条件独立，同样满足：$p(b/ac)=p(b/c)$。

     为什么要用呢？

     这是因为对于特征变量$x​$通常维数较高，每一个元素的取值可能有多个，那么$X​$的取值将是庞大的，例如，假设$x​$第$j​$个元素$x^{(j)}​$可能取值$S_j​$个，那么$x​$可能取值的个数：$\prod_{j=1}^{m}S_j​$

     由于：
     $$
     p(X=x/Y=C_k)=p(X^{(1)},...,X^{(m)}/Y=C_k)    \tag {12}
     $$
     那么条件概率$p(X=x/Y=C_k)$将会有同样庞大的的规模。如果$Y$的可取值有$K$个，那么要求出整个条件概率$p(X=x/Y=C_k)$分布，需要求出$K\prod_{j=1}^{m}S_j$个概率。

     另外，还有一点就是维数灾难，当维数升高时，需要庞大的样本数，才能支撑起估计出的概率的精度。而样本数目一般是有限的，很可能一些可以取到的值在样本中没有观察到，但是“未被观察到”与“该取值概率为0”通常是不同的。

* 参照：

  李航《统计学习方法》

  BISHOP《pattern recognition and machine learning》

  周志华 《机器学习》