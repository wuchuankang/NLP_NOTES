# 决策理论

## 决策理论：最小错误率和最小期望误差

* **最小错误率**

  这里是对分类问题而言的，现在考虑二分问题，对于一个输入向量，出现错误的情况是，真实标签是$C_1$的被分到$C_2$上，反之亦然。因为真实标签不知道，所以我们用概率来表达：
  $$
  \begin{split}
  p(mistake)
  &=p(\bold x\in R_1, C_2) + p(\bold x\in R_2, C_1) \\
  &=\int_{R_1}p(\bold x,C_2)d\bold x+\int_{R_2}p(\bold x,C_1)d\bold x 
  \end{split} \tag 1
  $$
  式$(1)$中，$R_i$意思是将$\bold x$分到$C_i$上的区域，并不是说积分区域是$R_i$，积分区域实际就是输入向量$\bold x$

  我们的决策理论是：**选择最大后验概率最大的!**

  可以通过下面的方式进行论证：
  $$
  if: \hspace 1cm {p(\bold x,C_1)>P(\bold x,C_2)} \tag 2
  $$
  那么我们就将一个输入向量$\bold x$分配到$C_1$中，此时：
  $$
  \begin{split}
  p(mistake)
  &=p(\bold x\in R_1, C_2) + p(\bold x\in R_2, C_1)\\
  &=\int_{R_1}p(\bold x,C_2)d\bold x+0\\
  &=\int_{R_1}p(\bold x,C_2)d\bold x
  \end{split}
  $$
  同样，当$\bold x​$分配到$C_2​$中时，$p(mistake)=\int_{R_2}p(\bold x, C_1)d\bold x​$，由于积分区域相同，都是输入向量$\bold x​$ ，显然，选择分配到$C_1​$时误差率更小。那为什么是最大后验概率呢？这是因为$p(\bold x,C_i )=p(C_i/\bold x)p(\bold x)​$，即对于不同的$C_i​$，$p(\bold x)​$是相同的，所以$p(\bold x,C_1)>P(\bold x,C_2)​$等价于$p(C_1/\bold x)>P(C_2/\bold x)​$。 

* **最小期望误差**

  对于一些分类问题，不同分错情况的影响结果是不同的，比如得癌症被判为正常，和正常被判为癌症，结果完全不同，前者可能没有得到及时治疗而死亡，后者可能影响不大，或者后续的检测可以检测出正常。通过一个惩罚函数，对某一种错分施加较大的惩罚，是的学习完成后，使得这种错分情况变得比其他情况要小，这样的学习才更有意义。

  对于一个输入，其真是的输出我们是不知道的，可以用联合概率来表达：$p(\bold x,C)$，每一种可能的真实输出都对应一个损失，我们用期望损失来表达整体的损失:
  $$
  \mathbb E(L) = \sum_i\sum_j\int_{R_j} L_{ij}P(\bold{x},C_i)d\bold{x}
  $$
  由于联合概率$p(\bold {x},C)$是未知的，所以我们用训练集（样本）来估计期望损失，也就是用经验损失来估计期望损失，然后优化经验损失函数。其具体过程在这里叙述一下：对于训练集$\left \{ (x_1,y_1),...(x_N,y_N) \right \}$，对每一个输入$x_i$，通过建模都可以预测出一个$f(x_i)$，其经验损失就是$L(y_i,f(x_i))\widehat P_{data}(x\in f(x_i),y_i)=\frac 1 N L(y_i,f(x_i))$，所以整个训练集的经验损失就是$\frac 1 N \sum_i^N {L(y_i,f(x_i))}$。

  那么决策阶段，也就是对一个新的输入应该指定一个类的依据是什么呢？我们的决策是，将其分类到这样一个类$R_j​$中，使得其损失函数最小，也就是下面的：
  $$
  \begin{split}
  R_{opt}
  &=\arg\min\limits_{R_j} \sum_i \int_{R_j} L_{ij}P(\bold x, C_i)d \bold{x}\\
  &=\arg\min\limits_{R_j} \int_{R_j} \sum_i L_{ij}P(C_i/\bold x) P(\bold x)d \bold{x}\\
  &=\arg\min\limits_{R_j} \sum_i L_{ij}P(C_i/\bold x)
  \end{split}
  $$

上面式子中对任意类别$C_i​$，$P(\bold x)​$是常值，这是因为$P(\bold x)​$就是输入特征的概率密度，与$C_i​$无关。可以看到最有化决策是：

**对于一个新的输入，将其分为$C_j$类，该类使得$\sum_i L_{ij}P(C_i/\bold x)$最小！**

* **最小错误率和最小期望误差的联系**

  考虑二分问题，当损失函数是$0-1​$损失时，这两个是相同的，具体分析见李航《统计学习方法》中第4章朴素贝叶斯方法：后验概率最大化的含义。现摘录于下：
  $$
  \begin{split}
  L(Y,f(X))=\left \{
  \begin{array}{rcs}
  1, & & Y\neq f(X)\\
  0, & & Y=f(X)
  \end{array}
  \right .
  \end{split}
  $$

式中$f(x)$是决策函数。这时的期望损失是：
$$
\begin{align}
R_{exp}(f)
&=E[L(Y,f(X))]\\
&=E_X[\sum_{k=1}^KL(C_k,f(X))P(C_k/X)]
\end{align}
$$
现在考虑$X$变量类型：

1. $X$是离散变量，此时
   $$
   E_X[\sum_{k=1}^KL(C_k,f(X))P(C_k/X)]=\sum_{i=1}^N\left\{\sum_{k=1}^KL(C_k,f(X=x_i))P(C_k/X=x_i))\right\}P(X=x_i)
   $$
   

   那么只需要对$X$变量所取值逐个极小化即可。举个例子，比如$X$是掷骰子所掷出的数，则$X=\left\{1,2,3,4,5,6\right\}$，则对每一个$i$都需要极小化$\sum_{k=1}^K L(C_k,f(X=i))P(C_k/X=i)$，才能使得期望损失最小化。

2. $X$是连续性变量，此时
   $$
   E_X[\sum_{k=1}^KL(C_k,f(X))P(C_k/X)]=\int_{-\infty}^{+\infty} \left\{\sum_{k=1}^KL(C_k,f(X))P(C_k/X)\right\}P(X)dX
   $$
   对于积分量$\sum_{k=1}^KL(C_k,f(X))P(C_k/X)P(X)$，$P(X)$是$X$的概率密度，对积分量而言是个常值，唯一使得积分量发生变化的是模型$f(X)$，对于一个$X$，模型参数的不同，得到的$f(X)$结果不同，也就使得$\sum_{k=1}^KL(C_k,f(X))P(C_k/X)$不同，所以要使得期望损失最小，则要使得$\sum_{k=1}^KL(C_k,f(X))P(C_k/X)$最小即可。

综上考虑，对于$0-1$损失函数，使得期望损失最小，就是求：
$$
\begin{align}
f(x)
&=\arg\min\limits_{y}\sum_{k=1}^KL(C_k,y)P(C_k/X=x)\\
&=\arg\min\limits_{y}\sum_{k=1}^K P(y\neq C_k/X=x)\\
&=\arg\min\limits_{y} 1-P(y=C_k/X=x)\\
&=\arg\max\limits_{y} P(y=C_k/X=x)
\end{align}
$$
说明一下，对于$P(y\neq C_k/X=x)$如何求？当$y=C_1$，那么$P(y\neq C_2/X=x)=P(C_2/X=x)$.

综合周志华的《机器学习》在来论述一下：

以多分类任务来解释决策理论，假设输出$y$有$N$中可能的类别标记，$y=(c_1,...,c_K)$，$\lambda_{ij}$是将一个真实标记为$c_i$的样本标记为$c_j$时产生的损失，基于后验概率$p(c_j|\bold x)$可获得期望损失：
$$
R(c_i|\bold x)=\sum_{j=1}^K\lambda_{ij}p(c_j|\bold x)
$$
我们的任务是寻找一个判定准则$h(x):x\rightarrow y$，以最小化总体风险：
$$
R(h)=E_{(\bold x,y)}[\lambda]=\sum_{\bold x}\sum_{j=1}^K\lambda_{ij}p(c_j|\bold x)p(\bold x)=E_{\bold x}[R(h(\bold x)|\bold x)]
$$
,如果我们能对$\bold x$的每一个取值最小化，那么总体风险就尅最小化，为了最小化总体风险，只需要在每一个可能取值上选择能使得条件风险$R(c|x)$最小的类别标记就可以了，即：
$$
h^{*}(x)=\arg\min_{c\in y}R(c|\bold x)
$$
