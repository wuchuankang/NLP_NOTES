##   指数族分布和广义线性模型

* 广义线性模型自然包括普通的线性模型，普通线性模型是：
  $$
  y=\bold w^T\bold x  \ \ \ \  or \ \ \  \  y=\bold W^T\bold x  \tag1
  $$
  广义线性模型一般是间接构造的，现在把构造的逻辑讲一下：

  构造的模型用$h_{\bold w}(\bold x)​$ 表示，即给定一个输入$\bold x​$，$h_{\bold w}(\bold x)​$给出预测。我们知道$h_{\bold w}(\bold x)​$应该是：
  $$
  h_{\bold w}(\bold x)=E( y|\bold x;\bold w)   \tag 2
  $$
  那么如果我们知道概率分布$p( y|\bold x;\eta(\bold w))$，当然这个概率分布有未知参数，用$\eta(\bold w)$表示，为什么$\eta$ 是$\bold w$的函数？其实不难理解，因为$E (y|\bold x;\bold w)$就是概率分布$p(y|\bold x;\eta)$的期望，显然$\eta$ 和$\bold w$是有函数关系的，可以用$\eta(\bold w)$来表示。当$\eta$ 和$\bold w$是线性关系时，即：$\eta(\bold w)=\bold w^T \bold x$，或者$\eta(\bold w)=\bold w^T \phi(\bold x)$，就是广义线性模型。

  那么问题就变为找$E( y|\bold x;\bold w)​$和$\eta​$的关系。

  就当前接触到的概率模型：bernulli分布，二项分布，beta分布，多项式分布，$Dirichlet​$分布，高斯分布，泊松分布都是指数分布族中的分布。

  为了书写方便，将$p( y|\bold x;\eta)$的$\bold x$省略，写成$p( y|\eta)​$。

  所以假设概率分布$p( y|\eta)​$ 是指数族分布，其定义：
  $$
  p(y|\eta)=b(y)\exp(\eta^TT(y)-a(\eta)) \ \ \  \ or \ \ \ \ p(y|\eta)=b(y)g(\eta)\exp(\eta^TT(y))\tag 3
  $$
  其中：$\eta$ 是自然参数，$T(y)$是充分统计量，一般$T(y)=y$，$a(\eta)/g(\eta)$是用于保证概率正交化的量，此处有$a(\eta)=-\ln g(\eta)$。

  1. bernoulli 分布：二分问题
     $$
     p(y|\mu)=Bern(y|\mu)=\mu^{y}(1-\mu)^{1-y}  \tag 4
     $$
     表达成指数形式：
     $$
     \begin{aligned}
     p(y|\mu)
     &=\exp(y\ln\mu+(1-y)\ln(1-\mu))\\
     &=(1-\mu)\exp(\ln(\frac{\mu}{1-\mu})y)
     \end{aligned}   \tag{4.1}
     $$
     可以看出：
     $$
     \eta=\ln{\frac{\mu}{1-\mu}}   \tag{4.2}
     $$
     Bern分布的期望：
     $$
     E(y|\eta)=1×\mu+0×(1-\mu)=\mu  \tag{4.3}
     $$
     由$(4.2)$可以得出：
     $$
     \mu=\frac{1}{1+\exp(-\eta)}=\sigma(\eta) \tag{4.4}
     $$
     注意$\sigma(\eta)$就是$sigmoid$函数。

     所以，我们建立的线性模型就是：
     $$
     h_{\bold w}(\bold x)=E( y|\bold x;\bold w)=\mu=\sigma(\eta(\bold w))=\frac{1}{1+\exp(-\bold w^T\bold x)}  \tag {4.5}
     $$
     这就是$logistical$模型。

  2. 多项式分布：多分类问题
     $$
     p(\bold y|\boldsymbol \mu)=\prod_{k=1}^{K}\mu_k^{y_k}=\exp \sum_{k=1}^K y_k\ln \mu_k=\exp(\boldsymbol \eta^T\bold y)  \tag{5}
     $$
     其中：$\boldsymbol \eta^T=[\ln\mu_1,...,\ln\mu_K] , \ \ \ \ \ \ \sum_{k=1}^K \mu_k =1​$

     可以看出：
     $$
     \eta_k=\ln\mu_k \Longrightarrow \mu_k=\exp(\eta_k)  \tag{5.1}
     $$

     $$
     \sum_{k=1}^K \mu_k =1 \Longrightarrow \sum_{k=1}^K \exp(\eta_k)=1  \tag{5.2}
     $$

     从而可以得到：
     $$
     \mu_k=\frac {\exp(\eta_k)}{1}=\frac {\exp(\eta_k)}{\sum_{l=1}^K \exp(\eta_l)} \tag{5.3}
     $$
     这就是$softmax$函数。

     多项式分布的期望是：
     $$
     E[\bold y|\boldsymbol \mu]=\sum_{\bold x}p(\bold y|\boldsymbol \mu)=(\mu_1,...,\mu_K)=\boldsymbol \mu  \tag{5.4}
     $$
     需要注意的是，$\bold y$ 用的是$one-hot$ 编码方式。

     所以，我们建立的线性模型，对每一个分量而言就是：
     $$
     h_{\bold w}^k(\bold x)=\mu_k=\frac {\exp(\bold w_k^T\bold x)}{\sum_{l=1}^K \exp(\bold w_l^T\bold x)} \tag{5.5}
     $$
     $h_{\bold w}^k(\bold x)$意思就是$\bold y$取第$k$类的概率，$(5.5)$就是$softmax \ \  regression$。

     这里说明一下$softmax \ \  function$:
     $$
     softmax(a_k)=\frac {\exp (a_k)}{\sum_j^K\exp(a_j)} \tag{5.6}
     $$
     它可以由贝叶斯定理推到出来：
     $$
     p(C_k|\bold x)=\frac {p(\bold x|C_k)p(C_k)}{\sum_j^K p(\bold x|C_j)p(C_j)}=\frac {\exp (a_k)}{\sum_j^K\exp(a_j)}=softmax(a_k) \tag{5.7}
     $$
     其中：$a_k=\ln (p(\bold x|C_k)p(C_k))$

  3. 高斯分布：用于回归问题

     当目标变量$y$是连续的，也即是面对的是回归问题的时候，如果，我们选择的概率模型是高斯分布，即$p(y|\bold x) \sim \mathcal N(y|\mu,\sigma^2)$，
     $$
     \begin{aligned}
     p(y|\mu,\sigma^2)
     &=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp(\frac{1}{2\sigma^2}(y-\mu)^2)\\
     &=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp(\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}y^2-\frac{1}{2\sigma^2}\mu^2)\\
     &=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp(-\frac{1}{2\sigma^2}y^2)\exp(\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}\mu^2)
     \end{aligned}   \tag{5.6}
     $$
     可以看出：
     $$
     \eta =\frac{\mu}{\sigma^2}   \tag{5.7}
     $$
     所建线性模型：
     $$
     h_{\bold w}(\bold x)=E( y|\bold x;\bold w)=\mu=\eta\sigma^2=\sigma^2\bold w^T\bold x=(\bold w')^T\bold x \tag {5.8}
     $$
     从(5.8)式，$\bold w'$只是$\bold w$的缩放，所以我们在建模的时候，直接写成$h_{\bold w}(\bold x)=\bold w^T\bold x$，不会影响结果。

+ 线性模型建立好后，如何估计出参数？

  对于$logistic$和$softmax$，因为其建立的线性模型仍是表示概率，所以可以用最大似然估计来估计参数。

  对于回归问题，就要给出损失函数，其实对于分类问题，也是给出损失函数的，只是分类问题中的最大似然和交叉熵是等价的，即用最大似然，就是对交叉熵进行优化。会说回来，回归问题的损失函数可以采用均方损失：
  $$
  L=\frac {1}{2}\sum_{n=1}^N(y-h_{\bold w}(\bold x))^2=\frac {1}{2}\sum_{n=1}^N(y-\bold w^T\bold x)^2   \tag6
  $$
  这和选择：
  $$
  y=\bold w^T \bold x+\epsilon\\
  \epsilon \sim \mathcal N(\epsilon|0,\sigma^2)\\
  p(y|\bold x,\bold w)=\mathcal N(y|\bold w^T \bold x,\sigma^2)
  $$
  负对数似然：
  $$
  -LL=\frac {1}{2\sigma^2}\sum_{n=1}^N(y-\bold w^T\bold x)^2+const  \tag{6.1}
  $$
  因为要优化的是$\bold w$，与$\sigma^2$无关，$\sigma^2$可以看出一个常值，故上式就变成了式$(6)$。

+ 链接函数与规范链接函数

  对于$(y,\bold x)$，其中$\bold x=(x_1,...,x_m)，y\in R $  服从指数分布(见式(3))：
  $$
  p(y|\bold x,\eta)=b(y)\exp(\eta y-a(\eta))\ \ \  \ or \ \ \ \ p(y|\eta)=b(y)g(\eta)\exp(\eta y)   \tag{7.1}
  $$
  这里$T(y)=y, \ \ a(\eta)=-\ln g(\eta)$ ，该式的补充说明见最后的补充。

  通过对$\eta$ 求导，可以得到：
  $$
  \triangledown a(\eta)=E[y|\bold x,\eta]=t  \ \ \ or \ \ \  \ -\triangledown \ln g(\eta)=E[y|\bold x,\eta]=t \tag{7.2}
  $$
  这里一个非常重要的特征就是$\triangledown a(\eta)=E[y|\bold x,\eta] ​$，我们最终的目标就是求$E[y|\bold x,\eta] ​$。

  我们的线性形式是：
  $$
  \theta=w_0+w_1x_1+...+w_mx_m=w^T\bold x  \tag{7.3}
  $$
  这里的$w=[w_0,...,w_m],\bold x=[x_0,x_1,...,x_m]$，与上面提到的$\bold x=[x_1,...,x_m]$的不同要注意到。

  由式$(7.2)$，可见$\eta$ 和预测结果$t$之间存在函数关系：
  $$
  \eta=\gamma(t)=\triangledown^{-1} (t)   \tag{7.4}
  $$
  其中$\triangledown^{-1} ​$是指的对$a(\eta)​$ 求导后得到导函数$\triangledown a(\eta) ​$的反函数。

  现在来看看我们建立的模型：
  $$
  t=E[y|\bold x,\eta]\\
  t=h_w(\theta)  \ \ \ or \ \ \ \theta=h_w^{-1}(t)  \tag{7.5}
  $$
  其中$h_w(\theta)​$在机器学习中叫激活函数，$h_w^{-1}(t)​$在统计学中叫链接函数

  由此可以得到：
  $$
  \theta=h_w^{-1}(t)=h_w^{-1}(\gamma^{-1}(\eta))   \tag{7.6}
  $$
  $\theta​$ 和$\eta​$存在的特殊函数关系就是：$\theta=\eta​$，此时：
  $$
  h_w=\gamma^{-1}   \tag{7.7}
  $$
  此时的$h_w^{-1}(\centerdot)$，也即$\gamma(\centerdot)$ 叫规范链接函数。

+ 现在重新审视一下$bernoulli$分布，看看规范链接函数和普通链接函数，并由此引出$probit regression$。

  由$(4.1-3)$：
  $$
  p(y|\mu)=(1-\mu)\exp(\ln(\frac{\mu}{1-\mu})y)\\
  E(y|\eta)=1×\mu+0×(1-\mu)=\mu\\
  \eta=\ln{\frac{\mu}{1-\mu}}=\gamma(E(y|\eta))=\gamma(\mu)
  $$
  如果$\theta=\eta$，也就是：$\eta=w_0+w_1x_1+...+w_mx_m=w^T\bold x$，

  此时规范链接函数就是：
  $$
  \gamma(\mu)=\ln{\frac{\mu}{1-\mu}}
  $$
  从而可以推导出$sigmoid$函数。

  如果$\theta \neq \eta$，也即$h_w \neq \gamma^{-1}$，此时
  $$
  t=E[y|\bold x,\eta]=\mu=h_w(\theta)   \tag{8.1}
  $$
  假如：
  $$
  t=h_w(\theta)=\int_{-\infty}^{\theta}\mathcal N(a|0,1)da=\int_{-\infty}^{w^T\bold x}\mathcal N(a|0,1)da=\Phi(\theta)  \tag{8.2}
  $$
  这个就是逆$probit$函数，$\Phi^{-1}(E(y|\eta))$是$probit$函数，链接函数也就是$probit$函数。

  从而对于二分类问题，当链接函数是$probit$函数时，其模型：
  $$
  p(t=1|\bold x, w)=\int_{-\infty}^{w^T\bold x}\mathcal N(a|0,1)da   \tag{8.3}
  $$
  这里总结一下思路：

  通过式$(7.2)$解出$E[y|\bold x,\eta]$，然后式$(8.1)$中，当然如果提前知道了是什么分布，那么求$E[y|\bold x,\eta]$可以不通过$(7.2)$式。

  $probit$模型会在求$bayesian logistic regression$用到。

+ 对指数分族分布的补充(参见式$(7.1)$)

  指数族分布的这种形式更易分析：
  $$
  p(\bold x|\boldsymbol \eta)=h(\bold x)\exp(T(\bold x)^T\boldsymbol \eta-A(\boldsymbol\eta))   \tag{9.1}
  $$
  $\boldsymbol \eta$叫自然参数，其中$T(\bold x)$是$\bold x$的向量函数，$a(\boldsymbol \eta)$叫正交化系数(normalizer)，这个可以从下面的式子看出：
  $$
  \int h(\bold x)\exp(T(\bold x)^T\boldsymbol \eta-A(\boldsymbol\eta))dx=\frac{1}{\exp(A(\boldsymbol\eta))}\int h(\bold x)\exp(T(\bold x)^T\boldsymbol \eta)dx=1   \tag{9.2}
  $$
  这里有一个好的性质，那就是
  $$
  \triangledown\,A(\boldsymbol \eta)=E_{p(\bold x|\boldsymbol \mu)}[T(\bold x)]   \tag{9.3}
  $$
  这个通过对$(9.2)​$进行求导数就可以得到。
  $$
  \begin{align}
  \frac{\partial \int h(\bold x)\exp(T(\bold x)^T\boldsymbol \eta-A(\boldsymbol\eta))dx}{\partial \boldsymbol \eta}
  &=\int\frac{\partial  h(\bold x)\exp(T(\bold x)^T\boldsymbol \eta-A(\boldsymbol\eta))}{\partial \boldsymbol \eta}dx\\
  &=\int \left\{h(\bold x)\exp(T(\bold x)^T\boldsymbol \eta-A(\boldsymbol\eta))\right\} (T(\bold x)-A^{'}(\boldsymbol \eta)) dx\\
  &=0
  \end{align}\\
  \Rightarrow A^{'}(\boldsymbol \eta)=\int \left\{h(\bold x)\exp(T(\bold x)^T\boldsymbol \eta-A(\boldsymbol\eta))\right\} T(\bold x)\,dx=E_{p(\bold x|\boldsymbol \mu)}[T(\bold x)]   \tag{9.4}
  $$
  对于指数族分布的对数似然函数
  $$
  L=\sum_{i=1}^{N}\ln p(\bold x_i|\boldsymbol \eta)=\sum_{i=1}^N \ln h(\bold x_i)+\sum_{i=1}^N (T(\bold x_i)^T\boldsymbol \eta-A(\boldsymbol \eta))     \tag{9.5}
  $$
  对上式关于$\boldsymbol \mu$ 求导，令其为0，可得
  $$
  A^{'}(\boldsymbol \eta)=\frac{1}{N} \sum_{i=1}^N T(\bold x_i)   \tag{9.6}
  $$
  可见参数的最大似然估计，只是和$\sum_{i=1}^N T(x_i)$有关，因此$\sum_{i=1}^N T(x_i)$也称为充分统计量。如果我们把高斯分布写成$(9.1)$式，那么其最大似然函数将很好求，因为写成$(9.1)$式，高斯分布对应的$A(\boldsymbol \eta)$就是已知的，其导数很容易求出，对于一元高斯分布
  $$
  \boldsymbol \eta=
  \left(
  \begin{matrix}
  \mu/\sigma^2\\
  -1/2\sigma^2
  \end{matrix}
  \right)  \tag{9.7}
  $$
  通过简单的转化，很容易参数$\mu, \sigma^2$的最大似然解，这要比直接对对数似然函数关于参数$\mu, \sigma^2$求导简单的多。

+ 参照：

  1.[[What is the difference between a “link function” and a “canonical link function” for GLM](https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function)]

​     