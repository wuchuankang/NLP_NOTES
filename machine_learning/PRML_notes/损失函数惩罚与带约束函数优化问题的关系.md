# 损失函数惩罚和带约束问题的函数优化之间的关系
我们知道在损失函数中添加权重参数的惩罚项，对参数进行约束，从而来避免过拟合问题。那么为什么直接添加惩罚就可以达到目的呢？  
这其实有两方面可以来推导：
  - 通过最大后验概率(MAP)，惩罚项就是参数的先验概率得到的，这个在其他笔记上已经说明过;
  - 通过对损失函数添加参数的约束，也就是从带约束问题的函数优化问题推导得到，本文主要讨论这个问题;

## 带不等式约束的函数优化问题  
我们先来看看带不等式函数的优化问题如何解决，从而顺利过渡到损失函数的约束上  
$$
\min_x f_0(x) \\
s.t. \ \  f_i(x) \leq 0 \ \ \  i=1,2,...,m   \tag{1}
$$

通过**拉格朗日数乘法**可以将约束放到优化函数中，从而转化为拉格朗日函数，拉格朗日法并不能保证将有约束问题完全转化为无约束问题。  
$$
L(x,\lambda) =f_0(x)+ \sum_{i=1}^m \lambda_i f_i(x)   \tag{2}
$$
求解(1)式转化为求解
$$
g(\lambda) = \min_x L(x, \lambda) \tag{3}
$$
那么$g(\lambda)$和(1)式的最优解$f_0(x^*)$之间的有什么关系呢？当$\lambda \geq 0$时，有以下关系 
$$
g(\lambda) = f_0(x^*) + \sum_{i=1}^m \lambda_i f_i(x^*) \leq f_0(x^*)   \tag{4}
$$
也就是说，当$\lambda \geq 0$时，$g(\lambda)$是$f_0(x^*)$的下限，那么，为了让下限最大的接近最优解，只要求$\max_{lambda} g(\lambda)$即可。总结一下就是，我们将原代约束的最优化问题转化为下面的问题：
$$
\max_{\lambda \geq 0} \min_x L(x,\lambda) \tag{5}
$$
(5)式被称为原问题的对偶问题，不论原问题是不是凸优化问题，转化为对偶问题后，将是一个凸优化问题。具体的分析可见boyd《凸优化》的第5章。  
那么如何求解这个问题呢？使用次梯度下降法：  
  - 一般先随机初始化 $\lambda^k$
  - 将$\lambda^k$带入到$\min_x L(x,\lambda)$，求解当前最优解$x^k$
  - 然后将$x^k$代入到$\max_{\lambda} L(x,\lambda)$，求解得到$\lambda^{k+1}$
  - 然后令$\lambda^k := \lambda^{k+1}$，判断终止条件，否则从第二步继续开始迭代。
要注意，通过拉格朗日法我们并没有把约束全部去掉，还有约束为$\lambda \geq 0$
## 损失函数加惩罚  
现在过渡到损失函数的问题就很容易解决了。
以线性回归为例子，损失函数有
$$
l(w) = \frac{1}{m}\sum_{i=1}^m (y_i-w^Tx_i)^2  \tag{6}
$$
现在我们想对权重参数$w$加上一个约束：
$$
||w||_p \leq C   \tag{7}
$$
当$p=1$就是1范数，当$p=2$就是2范数，带1范数的就是lasso回归，带2范数的就是bridge回归。当是2范数的时候，为了好处理，通常取二范数的平方，这样的改变，只要$C$相应的变大就可以了。  
以2范数为例，通过拉格朗日法，可转化成为对偶问题：
$$
\max_{\lambda \geq 0} \min_w L(w, \lambda)= \max_{\lambda \geq 0}\min_w \frac{1}{m}\sum_{i=1}^m (y_i-w^Tx_i)^2 + \lambda (||w||_2^2 - C)  \tag{8}
$$
我们这里的处理方式，并不是像上面用次梯度下降法去求解，而是将引入的参数$\lambda$当做超参数来处理，那么式$(8)$就可以转化为求解  
$$
\min_{w}\frac{1}{m}\sum_{i=1}^m (y_i-w^Tx_i)^2 + \lambda ||w||_2^2 \tag{9}
$$
当$\lambda$是某一个超参数的时候，同理，优化$(9)$也是原问题最优解的一个下界。  
可知，对于任意给定的超参数，优化$(9)$式不一定是原问题的最优下界，所以才有调参的问题。  
这就由添加约束问题转化为我们所说的添加正则化问题。
## 考察一下SVM
从损失的角度来推导一下SVM，假如找到了超平面$w^Tx+b=0$，那么属于正类别的训练样本满足：$w^Tx_i + b \geq 0$，属于负样本的满足$w^Tx_i + b \leq 0$，假设正样本$y_i=1$，负样本$y_i=-1$，那么综合起来，所有的样本的函数间隔满足：$y_i(w^Tx_i + b) \geq 0$，但我们认为只有距离超平面一定距离之后，函数损失为0，这里我们定位函数间隔必须大于1后，损失为0，否则损失为$1-y_i(w^Tx_i+b)$，综合起来，单个样本的函数损失可以写为$\max(0, 1-y_i(w^Tx_i+b))$，也就是合页损失。那么总的损失函数为:
$$
\min_{w,b} l(w) = \min_w \sum_{i=1}^m \max(0, 1-y_i(w^Tx_i+b))  \tag{10}
$$
通过拉格朗日法转化为对偶问题：
$$
\max_{\lambda \geq 0}\min_{w,b} L(w,\lambda) = \max_{\lambda \geq 0} \min_{w,b} \sum_{i=1}^m \max(0, 1-y_i(w^Tx_i+b)) + \lambda ||w||_2^2  \tag{11}
$$
同理，如果转化成为损失函数加惩罚项，那么也将${\lambda}$当做超参数处理：
$$
\min_{w,b} \sum_{i=1}^m \max(0, 1-y_i(w^Tx_i+b)) + \lambda ||w||_2^2  \tag{12}
$$
### SVM的损失函数与软间隔函数最优化问题的转化
求解$(12)$可以使用梯度下降法，但是有更快的方法，那就是使用SMO算法，但是使用SMO算法要先转化为软间隔函数最有问题。其实，SVM推导，就有两种思路，一个就是上面的损失的角度，另外一个就是使用软间隔的角度。我们不去推导软间隔，直接从带2范数惩罚的$(12)$式来推导。
$$
令\max (0, 1-y_i(w^Tx_i+b)) = \xi_i \\
因为 \xi_i是最大值，所以有：\\
    \xi_i \geq 0 \\
    \xi_i \geq 1-y_i(w^Tx_i+b) \\
(12)式变为：
\min_{w,b}\sum_{i=1}^m \xi_i + \lambda ||w||_2^2  \\
令 \lambda = \frac{1}{2C} \\
\Longrightarrow \min_{w,b}\frac{1}{C}(C \sum_{i=1}^m \xi_i +\frac{1}{2}||w||_2^2)   \\
\Longrightarrow \min_{w,b}(C \sum_{i=1}^m \xi_i +\frac{1}{2}||w||_2^2)   \\
$$
那么总结一下软间隔函数最优化：
$$
 \min_{w,b}(C \sum_{i=1}^m \xi_i +\frac{1}{2}||w||_2^2)   \\
s.t. \ \ \ \ \ \ \    \xi_i \geq 0 \\
    \xi_i \geq 1-y_i(w^Tx_i+b) \\
$$
同样转化为对偶函数问题，然后利用SMO算法，具体的过程可以参照李航的《统计学习基础》。
