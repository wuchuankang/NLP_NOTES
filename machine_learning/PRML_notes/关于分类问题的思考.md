# 关于分类问题的思考

[TOC]

分类问题有判别函数、概率生成模型、概率判别模型、贝叶斯模型。

## 概率生成模型

+ 给定新的输入$\bold x$，要预测$p(C_1/\bold x)$，概率生成模型的意思就是通过把生成数据的概率分布$p(y,\bold x)$求出来，然后根据贝叶斯定理就可以求出来了。

对于二分问题，
$$
p(C_1|\bold x)=\frac {p(C_1,\bold x)}{p(\bold x)}=\frac {p(\bold x|C_1)p(C_1)}{\sum_{k=1}^2 p(\bold x|C_k)p(C_k)} \tag 1
$$
但是这里有个难点就是求$p(\bold x|C_k)​$，比如，假设$\bold x​$是连续函数，$p(\bold x|C_k)​$服从某一分布，比如是高斯分布，$p(\bold x|C_k)\sim \mathcal N(\bold x|\boldsymbol \mu_k,\boldsymbol \Sigma)​$，我们通过最大似然估计出了$\boldsymbol \mu_k,\boldsymbol \Sigma​$，但是当我们要求$p(C_k|\bold x)​$时，需要求$p(\bold x|C_k)​$，假设$\bold x​$是$m​$维变量，即求：
$$
p(\bold x|C_k)=\int_{-\infty}^{x_1}...\int_{-\infty}^{x_m}\mathcal N(\bold x|\boldsymbol \mu_k,\boldsymbol \Sigma)dx_1...dx_m  \tag2
$$
这个积分是很难求的，即使换成其他分布，同样积分也是难求的。那么我们需要走另外的路，该怎么解决呢？

方案是：

假如$p(C_1/\bold x)​$还可以表示成某一函数，而这个函数的参数又可以用概率模型中的参数来表示，例如，如果概率模型是是高斯分布，那么函数的参数可以用高斯模型的参数$\boldsymbol \mu_k,\boldsymbol \Sigma​$来表示，那么就可以不用积分就求出来了$p(C_1/\bold x)​$，因为只要把新的输入带入到函数中就可以了。

那么问题转化为如何找这个函数，幸运的是，对于分类问题，天然就有这种函数，对于二分类：sigmoid函数，多分类：softmax函数。

下面说明推导一下为什么天然具备，其实来源于贝叶斯定理，假如是二分问题：
$$
p(C_1|\bold x)=\frac {p(\bold x|C_1)p(C_1)}{\sum_{k=1}^2 p(\bold x|C_k)p(C_k)}=\frac {1}{1+\frac{p(\bold x|C_2)p(C_2)}{p(\bold x|C_1)p(C_1)}}=\frac {1}{1+\exp(-\ln\frac{p(\bold x|C_1)p(C_1)}{p(\bold x|C_2)p(C_2)})}=\frac {1}{1+\exp(-a)}=\sigma(a)  \tag3
$$
其中：
$$
a=\ln\frac{p(\bold x|C_1)p(C_1)}{p(\bold x|C_2)p(C_2)}  \tag 4
$$
$\sigma(a)$就是$sigmoid$函数，如果我们让$a=w^T \bold x$，那么线性模型就建立起来了，也就是：
$$
p(C_1|\bold x)=\sigma(w^T\bold x) \tag 5
$$
而
$$
p(C_2|\bold x)=1-p(C_1|\bold x) \tag {5.1}
$$
这里要说明一下，既然找到了$p(C_1/\bold x)$的函数形式，那么直接用最大似然函数就可以把函数的参数求出来了，其实这就是概率判别模型，那这么说，似乎没必要用上面间接求法。其实当然有不同，因为优化的函数不同，概率生成模型一方面更具有解释性，另一方面，如果概率类型选择合适，效果会更好。

现在来说一下，如果是多分类问题，对应的函数形式是什么呢？
$$
p(C_k|\bold x)=\frac {p(\bold x|C_k)p(C_k)}{\sum_{j=1}^K p(\bold x|C_j)p(C_j)}=\frac{\exp(a_k)}{\sum_{j=1}^K\exp(a_j)}\tag{5.2}
$$
其中：
$$
a_k=\ln[p(\bold x|C_k)p(C_k)]   \tag{5.3}
$$
明显就是$softmax$函数。



+ 输入**特征连续**问题：高斯生成模型

  1. **先考虑二分问题**，当输入特征$\bold x$是连续变量时，假设$p(\bold x|C_k)\sim \mathcal N(\bold x|\boldsymbol \mu_k,\boldsymbol \Sigma)$，对与$C_1$和$C_2$，高斯分布使用相同的$\Sigma$，由$(4)$可以得出：
     $$
     \begin{aligned}
     w^T\bold x = a\\
     &=\ln\frac{ \mathcal N(\bold x|\boldsymbol \mu_1,\boldsymbol \Sigma)p(C_1)}{ \mathcal N(\bold x|\boldsymbol \mu_2,\boldsymbol \Sigma)p(C_2)}\\
     &=-\frac{1}{2}((\bold x-\boldsymbol \mu_1)^T\Sigma^{-1}(\bold x-\boldsymbol \mu_1)-(\bold x-\boldsymbol \mu_2)^T\Sigma^{-1}(\bold x-\boldsymbol \mu_2)+\ln\frac{p(C_1)}{p(C_2)})\\
     &=(\boldsymbol \mu_1^T -\boldsymbol \mu_2^T)\Sigma^{-1}\bold x-\frac{1}{2}\boldsymbol \mu_1^T\Sigma^{-1}\boldsymbol \mu_1+\frac{1}{2}\boldsymbol \mu_2^T\Sigma^{-1}\boldsymbol \mu_2+\ln\frac{p(C_1)}{p(C_2)} 
     \end{aligned} \tag 6
     $$
     以前没有提到$w^T\bold x$写法上的特殊性，那就是线性函数原本有一个偏差$w_0$，假设$\bold x$ 是有$m$个元素，那么$a=w_1x_1+...+w_{m}x_{m}+w_0$ ，为了能用内积统一表示，引进$x_0=1$，则$a=w^T\bold x$，尤其要注意这一点。上面$(6)$就没有注意到这个问题，因为式子左边的$w^T\bold x$中的$\bold x=[1,x_1,...,x_{m}]$ ，而式子右边的$\bold x=[x_1,...,x_{m}]$，我们前提假设了输入变量是$m$维度的。

     所以$(6)​$应该改为：
     $$
     w^T\bold x +w_0=(\boldsymbol \mu_1^T -\boldsymbol \mu_2^T)\Sigma^{-1}\bold x-\frac{1}{2}\boldsymbol \mu_1^T\Sigma^{-1}\boldsymbol \mu_1+\frac{1}{2}\boldsymbol \mu_2^T\Sigma^{-1}\boldsymbol \mu_2+\ln\frac{p(C_1)}{p(C_2)} \tag {6.1}
     $$
     由此：
     $$
     w=[(\boldsymbol \mu_1^T -\boldsymbol \mu_2^T)\Sigma^{-1}]^T=\Sigma^{-1}(\boldsymbol \mu_1 -\boldsymbol \mu_2)  \\
     w_0=-\frac{1}{2}\boldsymbol \mu_1^T\Sigma^{-1}\boldsymbol \mu_1+\frac{1}{2}\boldsymbol \mu_2^T\Sigma^{-1}\boldsymbol \mu_2+\ln\frac{p(C_1)}{p(C_2)}
     \tag{6.2}
     $$
     现在我们只要把$\boldsymbol \mu_1,\boldsymbol \mu_2,\boldsymbol \Sigma,p(C_1)$估计出来，就可将$w$估计出来，问题就解决了。

     如何估计呢？自然使用最大似然估计大法：

     假设：
     $$
     p(C_1)=\pi   \tag{7.1}
     $$
     则：
     $$
     p(C_2)=1-\pi   \tag{7.2}
     $$
     那么对于一个输入$\bold x​$来说，其联合概率就是：
     $$
     p(C_1,\bold x)=p(C_1)p(\bold x|C_1)=\pi \mathcal N(\bold x|\boldsymbol \mu_1,\boldsymbol \Sigma)\\
     p(C_2,\bold x)=p(C_2)p(\bold x|C_2)=(1-\pi) \mathcal N(\bold x|\boldsymbol \mu_2,\boldsymbol \Sigma)  \tag{7.3}
     $$
     那么似然函数就是：
     $$
     p(\bold t, \bold x|\pi,\boldsymbol \mu_1,\boldsymbol \mu_2,\Sigma)=\prod_{n=1}^N[\pi \mathcal N(\bold x|\boldsymbol \mu_1,\boldsymbol \Sigma)]^{t_n}[(1-\pi) \mathcal N(\bold x|\boldsymbol \mu_2,\boldsymbol \Sigma)]^{1-t_n}   \tag{7.4}
     $$
     这里要说一句，就是为什么之前的似然函数把输入特征$\bold x​$当做条件，即求$p(\bold t | \bold x)​$，这里的似然函数求得却是联合分布呢?

     那是因为之前模型，没有把输入$\bold x$当成随机变量，只是把$\bold x$当成参数而已，虽然$p(\bold t | \bold x)$这么写，那只是好说明输出$\bold t$是依赖特征$\bold x$。

     而当前的模型，已经把$\bold x$当做随机变量处理，似然函数自然要求联合分布了。

     **考虑多分类问题**，那么：
     $$
     \begin{aligned}
     a_k=\bold w_k^T \bold x+ w_{k0}
     &=\ln[p(\bold x|C_k)p(C_k)]\\
     &=-\frac{1}{2}(\bold x-\boldsymbol \mu_k)^T\Sigma^{-1}(\bold x-\boldsymbol \mu_k)+\ln p(C_k)+const
     \end{aligned}   \tag{7.5}
     $$
     可以看到，$(7.5)$式等号两边并没有对应关系，因为最后一项有$\bold x^T\Sigma^{-1}\bold x$，似乎就不对了。

     实际上，将$(7.5)$式带入到$(5.2)$式中，会发现$\bold x$的二次项和$const$是可以消除的，结果是：
     $$
     \begin{aligned}
     p(C_k|\bold x)
     &=\frac{\exp(a_k)}{\sum_{j=1}^K\exp(a_j)}\\
     &=\frac {\exp(\boldsymbol \mu_k^T\Sigma^{-1}\bold x-\frac{1}{2}\boldsymbol \mu_k^T\Sigma^{-1}\boldsymbol \mu_k+\ln p(C_k))}{\sum_j^m\exp(\boldsymbol \mu_j^T\Sigma^{-1}\bold x-\frac{1}{2}\boldsymbol \mu_j^T\Sigma^{-1}\boldsymbol \mu_j+\ln p(C_j))}
     \end{aligned}\tag{7.6}
     $$
     从上式，我们重新规划$a_k$，使得：
     $$
     \begin{aligned}
     a_k
     &=\bold w_k^T \bold x+ w_{k0}\\
     &=\boldsymbol \mu_k^T\Sigma^{-1}\bold x-\frac{1}{2}\boldsymbol \mu_k^T\Sigma^{-1}\boldsymbol \mu_k+\ln p(C_k)
     \end{aligned}\tag{7.5}
     $$
     对应关系明显，就不写了，可见有时要灵活处理。

     

     同样用最大似然故居，就可以解决问题 了。

  2. 当是**离散特征**时，即输入特征$\bold x​$是离散的时：

     假设$\bold x​$是$m​$维的，现在先考虑$\bold x​$的元素是二分的，就是$x_i​$只取$\left \{0,1\right\}​$。那么$p(\bold x|C_k)​$会有多少种情况？这是一种排列问题，每一个维度上都可以取两个值$\left \{0,1\right\}​$，所以会有$2^m​$次方种情况，但是受到所有情况概率和为1的限制，实际要求$(2^m-1)​$种情况，在加上$C_1、C_2​$两种条件，那么将有$2*(2^m-1)​$种概率要求，当$\bold x​$维度较大时，要计算这么多种概率，这显然是不可取的。

     那么怎么办呢？

     朴实无华的朴素贝叶斯就上场了。假设$\bold x$ 中的元素是关于类别$C_k$是条件独立的，即：
     $$
     p(\bold x|C_k)=\prod_{i=1}^m p(x_i|C_k)   \tag{8.1}
     $$
     我们知道$a、b$关于$c$条件独立：
     $$
     p(ab|c)=p(a|c)p(b|c)  \tag{8.2}
     $$
     现在来看看所要求的概率还有多少种呢？

     在类别确定的情况下，输入特征的每个元素都有两种可以取的结果，就是$2m$种，但是由于每个元素只要求出其中一个条件概率$p(x_i|C_k)$，另一种就不用求，直接用$1-p(x_i|C_k)$即可，所以有$m$个概率需要求；因为有2种类别，所以有$2m$个概率要求,显然个数极大缩减。

     我们现在假设是输入变量$\bold x$ 条件独立的，$p(x_i |C_k)=\mu_{ki}$，那么：
     $$
     p(\bold x|C_k)=\prod_{i=1}^m p(x_i|C_k)=\prod_{i=1}^m \mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i}   \tag{8.3}
     $$
     现在寻找函数参数$w$与模型参数之间的关系：
     $$
     \begin{aligned}
     w^T\bold x+w_0 = a\\
     &=\ln\frac{p(\bold x|C_1)p(C_1)}{p(\bold x|C_2)p(C_2)} \\
     &=\sum_{i=1}^m\left[x_i\ln\frac{\mu_{1i}(1-\mu_{2i})}{\mu_{2i}(1-\mu_{1i})}+\ln\frac{1-\mu_{1i}}{1-\mu_{2i}}\right]+\ln \frac{p(C_1)}{p(C_2)} \\
     &=\left[\ln\frac{\mu_{11}(1-\mu_{21})}{\mu_{21}(1-\mu_{11})},...,\ln\frac{\mu_{1m}(1-\mu_{2m})}{\mu_{2m}(1-\mu_{1m})}\right]\bold x+\sum_{i=1}^m\ln\frac{1-\mu_{1i}}{1-\mu_{2i}}+\ln \frac{p(C_1)}{p(C_2)}
     \end{aligned} \tag {8.4}
     $$
     明显的对应关系，在此不赘述。

     当是多分类问题时：
     $$
     \begin{aligned}
     a_k=\bold w_k^T \bold x+ w_{k0}
     &=\ln[p(\bold x|C_k)p(C_k)]\\
     &=\sum_{i=1}^m x_i\ln\mu_{ki}+(1-x_i)\ln(1-\mu_{ki}) +\ln p(C_k)\\
     &=\left[\ln \frac{\mu_{k1}}{1-\mu_{k1}},...\ln \frac{\mu_{km}}{1-\mu_{km}}\right]\bold x+\sum_{i=1}^m\ln(1-\mu_{ki})+\ln p(C_k)
     \end{aligned}\tag{8.5}
     $$
     其对应关系明显，在此不赘述。多说一句，就是多分类问题也可用于二分，二分就是其特例而已。

     那么，如果输入变量$\bold x$的每一个元素，取值个数也是多个，且每一个元素取值个数不一定相同，如何处理？

     依然假设条件独立，假设第$i$个元素可取值的个数是$n_i$个，那么：
     $$
     p(\bold x|C_k)=\prod_{i=1}^m \prod_{j=1}^{n_i}p(x_{ij}|C_k)=\prod_{i=1}^m \prod_{j=1}^{n_i}\mu_{ki}^{x_{ij}}\\  
     s.t. \ \ \ \prod_{j=1}^{n_i}p(x_{ij}|C_k)=1 \tag{8.6} 
     $$
     其中**$x_i​$是用$''one-hot''​$编码得到的向量**，那么：
     $$
     \begin{aligned}
     a_k
     &=\ln[p(\bold x|C_k)p(C_k)]\\
     &=\sum_{i=1}^m (\sum_{j=1}^{n_{i}} x_{ij}\ln\mu_{kij}) +\ln p(C_k)\\
     &=\sum_{i=1}^m ( \left[\ln\mu_{ki1},...,\ln\mu_{kin_i}\right]x_i) +\ln p(C_k)\\
     \end{aligned} \tag{8.7}
     $$
     

此时没法表达出像之前那样$a_k=\bold w_k^T \bold x+ w_{k0}$，但这并没有问题，我们只要求出了$\ln \mu_{kij}$，$ \  \  \ k=1,...,K;\  \  \  i=1,...,m;\  \  \  j=1,...,n_i$，那么$a_k, \ \  k=1,...,K$就都可以求出来，然后带入到$(5.2)$式，就可以求出后验$p(C_k |\bold x)$。 

+ 另外要说明一个问题：

  如何用基函数？即$a(\bold x)=w^T\phi(\bold x)​$或者$a_k(\bold x)=w_k^T\phi(\bold x)​$？当然我们考虑的特征是连续的，因为当$\bold x​$是离散的时候，没有所谓的基函数。

  答案是一样处理，只是现在把$\phi(\bold x)$当成自变量：
  $$
  p(\phi(\bold x)|C_k)= \mathcal N(\phi(\bold x)|\boldsymbol \mu_k,\Sigma)  \tag{8.8}
  $$
  其他类比就可以一样推出来了。


## 概率判别模型

​	对于二分类就是$Logistic \,\,regression​$，对于多分类问题就是$Softmax \,\,regression​$。

## 贝叶斯逻辑回归模型

$Logistic \,\,regression$的似然函数是：
$$
\ln p(\bold t|\bold w,\bold X)=\sum_{n=1}^N\left\{t_n\ln y_n+(1-t_n)\ln(1-y_n) \right\}
$$
其中$y_n=\sigma(\bold w^T\phi_n)$。

当我们将$\bold w$的先验采用高斯模型$p(\bold w)=\mathcal N(\bold w|m_0,S_0)$，
$$
p(\bold w|\bold t)\propto p(\bold w)p(\bold t|\bold w)
$$
显然后验不在是高斯模型，其概率模型也是很难求得，我们采用拉普拉斯近似，就是将后验概率$p(\bold w|\bold t)$近似为高斯模型。
