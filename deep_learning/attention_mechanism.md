## 注意力机制
    参见《神经网络与深度学习》
### 神经网络的理解和注意力机制的引入
神经网络可以存储的信息量叫做 **网络容量**，一般来说，利用一组神经元来存储信息时，其网络容量与 **神经元数量和网络复杂度成正比**。如果要存储越多的信息，那么神经元数量要越多或者是网络要越复杂，这导致网络的参数成倍的增加，从而需要大量的计算资源。  
为了减少计算复杂度，引入了 **局部连接(CNN)、权重共享(CNN、RNN)** 和 **汇聚操作(CNN)** 来简化网络结构，但我们仍然希望在不增加模型复杂度( **主要是模型参数**) 的情况下，来提高模型的表达能力。  
比如用同一套网络(模型复杂度相同) `RNN` 来解决文本分类问题和阅读理解问题，前者是可行的，因为只要需要编码对分类有用的信息，用一个隐向量来表示文本语义是可行的。但是在阅读理解任务，这个编码向量很难反应文章的所有语义，另外编码时，还不知道可能会接收到什么样的问句，而这些问句可能涉及到文章的所有信息点，因此丢失任何信息都可能导致无法正确回答问题。那么就要提高模型的复杂度。  
有没有方法，在尽量不提高模型复杂度的前提下，增加网络容量呢？  
可以借鉴人脑处理信息的机制， **人脑的工作记忆大概只有几秒钟的时间，类似与神经网络中隐状态**，而人脑接收来自外部的信息相当的庞大，人脑在有限的计算资源下，通过 **注意力机制** 来有效的处理过载的信息。神经网络可以引入注意力机制，提高神经网络存储信息的容量。  
**在计算资源资源有限的情况下**，注意力机制作为一种有效的计算资源分配方案，将计算资源分配到更重要的任务，是解决信息超载的主要手段。  

### 注意力机制和自注意力机制

其实 **最大汇聚(Max Pooling)、门控(Gating)** 都可以近似看作是一种注意力机制；而此处要将的注意力机制，是直接聚焦式的将注意力分布到与任务相关的表示上。给定一个与任务相关的查询向量$q$，从$N$个输入向量$[x_1,...,x_N]$中选择出和某个特定任务相关的信息，这里的 $x_i$ 可以是一篇文档中第 $i$ 个词的特征表示(词向量)。那么如何找到与$q$相关信息的特征向量呢？这里使用的是 `软性选择` 机制，也就是对所有的特征向量都给一个权重，相关的特征向量通过学习后，会给较大的权重，这个权重通过 **相似度**打分，然后通过 `softmax` 函数压缩成权重：
$$
\alpha_i = softmax(s(x_i, q)) = \frac{\exp(s(x_i,q))}{\sum_j^N \exp(s(x_j, q))}
$$
其中$s(x_i, q)$就是相似度打分函数，一般使用下面两种：
- 加性模型： $s(x_i, q) = v^T \tanh(Wx_i + Uq)$
- 点积缩放模型： $s(x_i, q) = \frac{x_i^T q}{\sqrt d}$

上面所论述的查询向量$q$ ，是借用了文献检索时所使用的概念，文献检索是这样的，文件信息是以键值对$(K,V)$的形式存储的，比如文件中某一部分的信息是这样的：`(中国总人口数：14亿)`，当我们输入查询： `中国人口`，通过比对(比较相似度)文件中的各个键，发现和 `中国总人口数` 这个键相似度最高，直接就输出该键所对应的值就可以了。  
对应于注意力机制问题，S

