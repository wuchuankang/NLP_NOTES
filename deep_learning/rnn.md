## RNN

`RNN(recurrent neural network)` 循环神经网络，一般是指 `LSTM` 网络以及其变种。它是在简单的循环神经网络的基础上发展而来的。  
循环神经网络具有短期记忆能力，因为当前的隐状态或者输出，不光是当前时刻的输出有关，还与上一个时刻的隐状态有关，将 `t` 时刻的隐状态展开到最初的 `1` 时刻，就会发现，过去的所有时刻的输入都以一定的权重整合到当前的隐状态中。  
但是为什么说是只有 `短期记忆能力` 呢？ 这是因为当输入序列比较长时，会存在 **梯度爆炸/梯度消失问题**，也称 **长程依赖问题**，缓解(无法解决)问题的手段是引入 **门控机制**，也就是 `LSTM`以及变种形式。  

### 简单循环神经网络

当前 `t` 时刻的输入是 $x_t$，隐状态(隐藏层状态：`hidden state`)是 $h_t$，那么当前隐状态和过去隐状态和当前输入的关系  
$$
h_t = f(U_t h_{t-1} + W_t x_{t} + b)   \tag{1}
$$
- 其中 $h_0$ 一般初始化为零向量；  
- $U_t、W_t、b$ 是在所有的时刻共享的，故上式也可以写成如下；这个和 `CNN` 类似，其意义也是类似，都是为了获得一定的规律特征，比如在 `nlp` 应用中，就是为了学习一定的语法特征。  
    $$
    h_t = f(U h_{t-1} + W x_{t} + b)   
    $$
    
- $h_t$维度在各个时刻都是相同的，权重参数又是共享的，因此我们只要确定了$x_t$和$h_t$的维度，那么参数的维度就确定了。  
- $f$ 是激活函数，它对**各层的神经元逐个的进行操作**，常见的激活函数有
    1. `sigmoid` 形函数， 如`logistic`函数(也叫`sigmoid`函数)、`tanh`函数
        $$
        \sigma(x) = \frac{1}{1+\exp(-x)} \\
        \tanh (x) = 2\sigma(2x) - 1
        $$
        - sigmoid 函数的输出可以看做实际概率分布，使得神经网络可以和统计模型相结合，如构造损失函数；  
        - 可以当做一种`软性控制们`，来控制**其他神经元输出信息的量**，之所以说是软，不是让某些神经元不输出信息，而是以一定的权重输出信息，从而有侧重的输出信息。这在 `LSTM` 中用到；
        - sigmoid 形函数两端易趋于饱和，也就是两端的梯度容易为`0`，易造成`梯度消失问题`，当我们带入一个样本，在某一层的某个神经元在所有的训练数据上，不是过大就是过小，也就是在梯度趋于`0`的地方，那么该神经元的参数就不能被更新，也就不能被学习到。当然一般不会再所有的数据上都是在梯度趋近`0` 的地方，但是如果大部分是，那么该神经元的参数更新的次数就少，从训练数据中学习到的信息就少，从而影响到学习到的模型的性能。而使用其他的激活函数可能会好很多，这也是 `CNN` 中一般不用 `sigmoid` 形函数的原因，一般用`ReLU`。
    
    2. 'ReLU函数以及变种'
        $$
        ReLU(x)=\left\{
        \begin{aligned}
        x && x \ge 0 \\
        0 && x<0 \\
        \end{aligned}
        \right.
        $$
        - 采用 `ReLU`，神经元只需要简单的加减乘除运算，计算量少；
        - 更具生物上的解释性，在生物神经网络中，同时处于兴奋的神经元非常的少，而使用`ReLU`，大约会有 $50\%$ 神经元处于抑制。  
        - 某个隐藏层的某个`ReLU` 神经元(采用 `ReLU`函数的神经元)在所有的训练数据上都不能激活，那么该神经元自身参数的梯度永远都是0，就没法更新，这个叫 `死亡ReLU问题`。
    3. 网络图中的神经元代表综合的含义：**既是上一层神经元的激活后的矩阵变换(净活性值)，也是该变换后的应用激活函数后的激活值**，另外， **一个神经元代表一个一个特征**

- 容易发现，简单循环神经网络容易出现 `长程依赖问题`

### LSTM 和 GRU

- LSTM 引入了一个**新的内部状态$c_t$，专门进行线性的循环信息的传递，同时非线性的输出信息给隐藏层的外部隐状态$h_t$**  
    $$
    c_t = f_t \odot c_{t-1} + i_t \odot \hat c_{t}  \\
    h_t = o_t \odot \tanh \hat c_t \\
    
    \hat c_t = \tanh(U_c h_{t-1} + W_c x_t + b_c)     \\
    f_t = \sigma (U_f h_{t-1} + W_f x_t + b_f)    \\
    o_t = \sigma (U_o h_{t-1} + W_o x_t + b_o)    \\
    i_t = \sigma (U_i h_{t-1} + W_i x_t + b_i)      \tag{2}
    $$

    - 其中 $U_c、W_c、b_c$ 各个时刻共享，其他的参数也各自一样。
    - $f_t$ 是遗忘门，控制从上一个时刻内部隐状态获取的信息量
    - $i_t$ 是输入门，控制从当前时刻的候选状态 $\hat c_t$ 中获取的信息量
    - $o_t$ 是输出门，控制从当前时刻的内部隐状态获取多少信息

- GRU(gated recurrent unit)：门控循环单元

    - 引入了更新门和重置门，在 `LSTM` 中输入门和遗忘门具有的互补关系，具有一定冗余性。  
        $$
        h_t = z_t \odot h_{t-1} + (1-z_t) \odot \hat h_{t}  \\  \tag{3} 
        z_t = \sigma(U_z h_{t-1} + W_z x_t + b_z)   \\
        \hat h_t = \tanh(U_h (r_t \odot h_{t-1}) + W_h x_t + b_h)   \\
        r_t = \sigma(U_r h_{t-1} + W_r x_t + b_r)
        $$
        - $z_t$ 是更新门，控制从上一个时刻的隐状态中获取的信息量；
        - $r_t$ 是重置门，控制候选状态$\hat h_t$中从是一时刻的状态中获取的信息量；

    - 候选状态，在 `LSTM` 中是内部候选隐状态 $\hat c_t$，在 `GRU` 中是候选隐状态 $\hat h_t$，候选状态都是由当前时刻的输入 $x_t$ 和 上一个时刻的 隐状态得到的。 

### 深层循环神经网络 
参见《神经网络与深度学习》$p_{156}$
- 堆叠循环神经网络(stacked recurrent neural network)，增加 $x_t \rightarrow h_t$ 的深度(层数)
    $$
    h_t^{(l)} = f(U^{(l)} h_{t-1}^{(l)} + W^{(l)} h_t^{(l-1)} + b^{(l)})       \tag{4}
    $$
    **注意参数共享和各层的隐状态的维度一般都是相同的，这样通过控制输入$h_t^0 = x_t$的维度，就可以确定各个层的参数的维度。**

- 双向循环神经网络(`Bi-RNN`)，第一层按时间顺序，第二层按时间逆序  
    $$
    h_t^{(1)} = f(U^{(1)} h_{t-1}^{(1)} + W^{(1)}x_t + b^{(1)})   \\
    h_t^{(2)} = f(U^{(2)} h_{t+1}^{(2)} + W^{(2)}x_t + b^{(2)})   \\
    h_t = h_t^{(1)} \oplus h_t^{(2)}    \tag{5}
    $$
    $\oplus$ 是向量拼接操作。可以是将向量直接拼接在一起，也可以是通过线性变换的方式操作；
    
- 多层双向神经网络，是在 `Bi-RNN` 的基础上堆叠神经网络，一般是双向双层神经网络。

### RNN 应用到机器学习任务
参见《神经网络与深度学习》$p_{144}$  
- 序列到类别模式：如文本分类，将最后一个时刻的隐状态作为整个序列的特征表示，也可以将整个序列的隐状态平均作为序列的表示，然后通过softmax 构建分类器。  
- 同步的序列到序列模式： 如序列标注问题，如 词性标注(part-speech tagging)

- 异步的序列到序列模式： 也称`编码器-解码器`模型，如机器翻译、文本摘要。

  

