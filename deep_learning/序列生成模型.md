## 序列生成模型

### 序列生成模型 与 基本解决路径

自然语言处理中，一个句子可以看做是符合一定自然规则的序列，文本也可以看做是一个序列。  
将一个长度为 $T$ 的文本序列看做成一个随机事件 $X_{1:T}=<X_1,...,X_T>$，其中每一个位置上的变量 $X_t$ 的样本空间为一个给定的词表 $V$ ，那么整个序列的样本空间就是 $|V|^T$ ， 这将是一个相当大的空间。那么当我们求长度为 $T$ 序列的概率分布的时候，其分布表 $S$ 将相当的庞大，另外加上样本序列长度不固定，其固定的概率分布表也是没有的，要学习的是一系列的概率分布表，比如不同长度的序列，是有不同长度序列的随机事件依概率分布产生的。那么不同长度的序列有没有 **共同的模式** 呢？  
序列模式每一个时刻可能的取值是相同的，以文本来说，序列中每一个位置可能的取值空间都是 `词表` $V$， 利用这一特性和概率的链式法则：
$$
p(x_{1:T}) = p(x_1)p(x_2|x_1)p(x_3|x_{1:2})...p(x_T|x_{1:(T-1)})    \tag{1}
$$
**因为序列中每一个位置的取值空间相同，对于不同的长度序列的随机事件，假设最长为$T_{max}$，最短的为 $T_{min}$，其 $P(x_1)、P(x_2|x_1)、p(x_3|x_{1:2})、...、p(x_{T_{min}}|x_{1:(T_{min}-1)})$ 这些概率分布是相同的，对于较长的序列，各自比 `最短序列` 多出一些条件概率，因此我们对序列分布不是直接求离散分布表，而是求这些条件概率，当给出一个新的序列，只要长度不超过 $T_{max}$ ，都可以通过概率的链式法则求出其概率来。**     

**所以语言模型就是求 $p(x_t|x_{1:(t-1)})$！！！**   
**如果序列满足 $n-1$ 阶马尔科夫模型，那么语言模型就是求$p(x_t|x_{(t-1):(t-n+1)})$！！！**

给定 $N$ 个序列的数据集 $\{x_{1:T_n}\}_{n=1}^N$，序列概率模型需要学习一个模型 $p_{\theta}(x_t|x_{1:t-1})$，来最大化整个数据集的对数似然概率：
$$
\max_{\theta} \sum_{n=1}^N \log p_{\theta}(x_{1:T_n}^n) = \max_{\theta} \sum_{n=1}^N \sum_{t=1}^{T_n} \log p_{\theta}(x_t|x_{1:t-1})    \tag{2}
$$
$(2)$ 叫 `自回归模型`，因为每一步都要将前面的输出都做后面的输入，是一种 `自回归`。  
$(2)$ 给出了基本的解决思路，但最大似然函数中的条件概率 $p_{\theta}(x_t|x_{1:t-1})$ 如何构造呢？只有给出了该条件概率的形式，才可以对 $(2)$ 进行最优化，从而学习到参数。  
具体有两种主流的方法： **N 元统计模型** 和 **深度序列模型** 。  

###  N 元统计模型

由于数据系数问题，当 $t$ 比较大的时候，很难估计条件概率 $p(x_t|x_{1:t-1})$ ，对序列问题进行约束，最简单的假设序列满足 `n阶马尔科夫性质`，即当前时刻的状态(或者说取值)只依赖前面 $n$ 个时刻的状态，而与其他状态(再往前的状态无关)， `0阶马尔科夫模型` 对于 `一元模型`， `1阶马尔科夫模型` 对应于 `二元模型`。  

**序列在概率中，可以看做是离散随机向量！而离散随机向量一般假设服从 多项分布！ 因此可以假设序列概率模型 $p(x_t|x_{1:(t-1)})$ 是服从多项分布！**
- 一元模型:  每一个元素都是独立的，每一个位置上的词都是从 `多项分布` 独立产生的，多项分布中，每个词产生的概率 $\theta = [\theta_1,...,\theta_{|v|}]$，那么似然函数中的 $p(x_t|x_{1:t-1})$ 
    $$
    p(x_t|x_{1:t-1}) = \prod_{i=1}^t p(x_i) = \prod_{k=1}^{|V|} \theta_k^{m_k}
    $$
   求解的具体过程见《神经网络与深度学习》。 
- N 元统计模型： 同理。  

### 深度序列模型

`Deep Sequence Model` 是指利用 **神经网络来估计条件概率 $p_{\theta}(x_t|x_{1:(t-1)})$。**   
同样假设，其满足 `n-1阶马尔科夫性质`，也就是`n元模型`，那么就是通过神经网络来拟合出条件概率 $p(x_t|x_{t-1:t-n+1})$，该模型最初的提出者就是 `Yoshua Bengio` 《A Neural Probabilistic Language Model》,具体求法见笔记《word2vec理论理解》。  
有语言概率模型发展出了词向量(word2vec)，而我们所说的 `word2vec` 是通过求语言概率模型而得到的词向量，具体有两种形式的方法 `skim-gram` 和 `cbow` 模型， 其中又使用了 **层次化 `softmax`** 来加速训练。


