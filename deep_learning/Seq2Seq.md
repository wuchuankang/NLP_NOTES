## 序列到序列模型(Seq2Seq)

### Seq2Seq 动机与方法
序列问题有 `学习问题` 和 `生成问题`。 Seq2Seq 是一种 **条件的序列生成问题**，给定一个序列 $x_{1:S}$，生成另一个序列 $y_{1:T}$。比如翻译问题、摘要问题。    
同《序列生成模型》中序列概率模型求的是 $p(x_t|x_{1:(t-1)})$，序列到序列模型的目标是估计：
$$
p_{\theta}(y_{1:T}|x_{1:S}) = \prod_{t=1}^T p_{\theta}(y_t|y_{1:(t-1)}, x_{1:S})    \tag{1}
$$
**所以目标转化为求　$p_{\theta}(y_t|y_{1:(t-1)}, x_{1:S})$。**  
其中 $y_t \in V$ 是词表 $V$ 中的某个词。    
给定一组训练数据 $\{(X_{S_n}, Y_{T_n})\}_{n=1}^N$，使用最大似然估计来训练模型参数：
$$
\max_{\theta} \sum_{n=1}^N \log p_{\theta} (y_{1:T_n}|x_{1:S_n})    \tag{2}
$$
训练完成后，根据一个新的输入序列　$x$　来生成最可能的目标序列：
$$
\hat y = \arg\max_{y} p_{\theta} (y|x)
$$
具体 $y$ 的生成，可以通过　**贪婪方法**　或者　**束搜索**　来完成。　　

### Seq2Seq 模型
和一般的序列生成模型类似，条件概率 $p_{\theta}(y_t|y_{1:(t-1)}, x_{1:S})$ 可以使用 **各种不同的神经网络**　来实现。
主要有 `3` 种主要的方法：
- 基于循环神经网络的序列到序列的模型
- 基于注意力的序列到序列模型
- 基于自注意力的序列到序列模型

#### 基于注意力的序列到序列模型 
使用两个 `RNN` 分别来进行 `编码` 和 `解码`， 也称 `编码器-解码器(Encoder-Decoder)` 模型。  
- 编码：用 `RNN` 来编码 输入序列 $x_{1:S}$ 得到一个固定维度的隐向量 $u$，一般 $u$ 是最后时刻的隐状态。  
- 解码：用另一个 `RNN` 来解码，假设解码器的隐状态用 $h^d$ 表示，将编码器得到的 $u=h_0^d$ 即可。  
缺点：
    - 编码向量 $u$ 的容量问题，很难将一个序列的全部信息保存在一个固定维度的向量中。
    - 当序列长时，会存在 `长程依赖问题`，**容易丢失序列信息。**  

#### 基于自注意力的序列到序列模型
为了缓解 编码向量 $u$ 容量问题，我们不是简单的选取最后时刻的隐状态作为输入信息编码，而是使用注意力机制来解决问题。  
在解码第 $t$ 步的时候，将上一步的隐状态 $h_{t-1}^d$ 作为 `查询向量` $q$， 将编码器所有序列的隐状态 $H^e=[h_1^e,...,h_S^e]$ 当做 `键值` $k、v$。从而在 $H^e$ 中选择相关信息，然后和 编码器的输入特征 $e_{y_{t-1}}$ 进行向量拼接，作为共同的输入。 
$$
c_t = \sum_{i=1}^S \alpha_i h_i^even    \\
\quad \quad \quad \quad \quad \quad \quad = \sum_{i=1}^S softmax(s(h_i^e, h_{t-1}^d)) h_i^e
$$
解码器第 $t$ 步的隐状态
$$
h_t^d = f_{dec}(h_{t-1}^d, [e_{y_{t-1}}; c_t], \theta_{dec})
$$
其中 $[e_{y_{t-1}}; c_t]$ 就是上面说的向量拼接。  

#### 基于自注意力的序列到序列模型

基于 `RNN` 的 `Seq2Seq` 的一个很大的不足点就是 **无法进行并行计算**，为了能够使用 **并行以及捕捉长距离依赖关系**，可是使用 **自注意力模型** 来建立一个 **全连接** 的网络结构。  
其中一个典型的基于自注意的 `Seq2Seq` 模型是 **Transformer**，注意， `Transformer` 只是其中一种！    
具体参见《神经网络与深度学习》  





