# seq2seq 模型 文本预处理要求

## 基本套路

- 去除噪音(即特殊的字符和标点符号):但是对于summary，要保留标点符号，因为summary可能不只有一句话，需要在测试的时候自己生成标点符号，如果去掉，那么测试的时候就不会生成标点符号，造成语句的不通顺

- 去除停用词：
    同样，只在text 中去除，不在summary中去除，为了保证语句的通顺

- 繁体转简体

- 分词

- 建立词典：对于低频词语，不能录入到词表中

- 确定文本长度的范围: 为每一个文本添加eos， 需要统计text 和 summary 的长度的分布加以确定

- 筛选文本: 过短和过长的text要删除，unk 过多的text要删除

- 使用预训练的词向量: 原始词向量的文件txt文件，每一行第一个元素是词，后面是向量，元素之间用空格分开的

- 在pytorch中，embedding 是依靠词的索引来查表的，所以要建立与词汇表同等大小的向量表，这是一个tensor, 维度是[vocab_size, embedding_dim]，第一行代表词表中索引为0的词的词向量，以后行，以此类推。

- 对于循环网络来说，文本序列长度，有两种处理方式：
    - 所有文本序列等长
    - 批文本序列等长，每一批不要求等长

- 批量化，需要构造批数据迭代器。

## 注意事项

- 可以使用 torchtext 包来简化处理过程


