# 卷积操作
神经网络中的卷积操作，借鉴了信号处理中的卷积操作：
$$
s(t) = \int x(a)w(t-a)da
$$
其离散形式是：
$$
s(t) = \sum_{a=-\infty}^{\infty} x(a)w(t-a)
$$
将a<0，x(a)=0，当t-a<0，x(a)=0，那么卷积可以理解为，每隔a时长，发送一个信号，信号会发生衰减，比如到时刻$t_1$，信号强度是原来的$w(t_1)$倍，那么时刻t的时候，通道中总的信号就是s(t)。  
联系到卷积神经网路中的卷积操作，输入数据通常是多维数据，核通常是通过优化的得到的参数，输入和核是分开存储的，如果假设存储了  
给定一个图像$X\in R^{M\times N}$，核为$W\in R^{m\times n}$：
$$
s(i,j) = \sum_{u=0}^{n-1} \sum_{v=0}^{m-1} x(u,v)w(i-u, j-v)
$$
因为卷积是对正负无穷之间进行操作，我们把核与x以外的数值范围当做是0，那么其卷积就是上式了。还有一个要理解的是，随着u,v的增大，w的索引变得越来越小，这里的索引并不是说(1,1)位置就对应了w中的(1,1)位置上的元素，而是i-u,j-v最小的是第一索引，所以当(u=0,v=0)的时候，x取图像X中第(i,j)个和W同样大小的块中的第一个元素，而w取最后一个，即$W_{m-1}^{n-1}$位置处的元素。所以，正常的情况下，对图像进行卷积的时候，不是对应位置元素相乘，而是对核进行翻转后，再对应位置相乘再求和。但是！我们为何不一开始就将卷积核取其翻转，也就是我们默认我们取的核$W$就是原来核翻转过的，那么操作就变化为：
$$
s(i,j) = \sum_{u=0}^{n-1} \sum_{v=0}^{m-1} x(u,v)w(i+u, j+v)
$$
因为卷积是可交换的，可以写成更通用的格式：
$$
s(i,j) = \sum_{u=0}^{n-1} \sum_{v=0}^{m-1} x(i+u,j+v)w(u, v)
$$
而这个正是互相关运算，互相关核卷积的区别就是不用对权重进行翻转，写出表达式就明白了:
$$
s(t) = \int x(a)w(t+a)da
$$
所以卷积神经网络核信号中的模式识别中的卷积是稍微有点区别的，用互相关来代替卷积。
神经网络中的卷积(也就是互相关)的表达式为：
$$
Y = W\otimes X
$$


