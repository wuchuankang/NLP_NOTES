# Deep contextualized word representations 结构化解读

## 摘要

- 研究任务：bert 是一个预训练模型

- 研究方法：该模型所有layer 都是依赖前后文的双向表示，且训练集不需要标注

- 实验效果和结论：在多个应用领域都取得了 state of art 的水平

## 介绍

- 研究任务： 预训练的词表征

- 已有方法： 前面已经有很多词表征的工作

- 主要挑战： 训练出高质量的词表征，包括：
    - 词的复杂特征：句法和语义上的
    - 随着文本而改变语义：**词的多义性(多义词)**，这是一个大问题，以我们用的Word2vec问题来说，训练出'bank'的Word embedding只有一个，但训练出的该词向量是用来表示 '河边' or '银行'，传统的词向量是解决不了这个问题到。更具体的说为什么Word2vec解决不了这个问题，那就是我们是用bank周围的词来预测bank的，但是周围词完全不同(语义不同)，但是都预测出同一个词bank，而bank这个词向量只有一行参数来表示(如果用多个向量来表示一个词的词向量,似乎能解决这个问题，这个想法就是论文中在related work中之处的 learning separate vectors for each Word)，这导致两种不同的上下文信息编码到相同的word embedding 中，所有用此方法的Word embeddding 无法区分多义词。

- 解决思路： 提出一个新型的**深度上下文词表征** 模型，解决上面的两个挑战，并且很容易整合到现有的 task-specific model，对一系列的语言理解问题都达到了 state-of-the-art。

- 具体方法：在大文本语料上使用多层双向lstm，目标函数是最后一层的双向lstm从左向右对应的语言模型和从右向左的语言模型的和，这个模型被称为ELMo，ELMo表征之所以是deep，是因为该表征是所有 biLM 层输出隐状态的函数。更具体的说就是各层隐状态的线性组合。而传统的是采用最后一层lstm 的隐状态。  
    内部隐状态的组合能够使得词具有丰富的表征。实验表明：high-level lstm states 能够捕捉词在上下文方面的意义(这可以用于消除词歧义性的问题)，而 lower-level states 捕捉到词在句法层面的意义(这可以用在 part-of-speech tagging)。
    **本文的词表征不同于传统的词表征在于：每一个词表征是输入句子整个序列的函数，而传统的是使用的窗口所包含词的函数**


- 实验结果： 在各方面的应用以及效果

## 相关工作

- 传统的词向量像 word2vec 能够从大语料中捕捉句法和语义信息，但是学到的每一个词的表征是和上下文无关的(这就没有捕捉到语序上的信息)；之后提出的方法从克服了传统的词向量的某些缺点：有的是利用了 subword(子词)信息，有的是给每个词训练出多个词向量。


## 文章依赖

bert -> deep contextualized word representations 

-> 
